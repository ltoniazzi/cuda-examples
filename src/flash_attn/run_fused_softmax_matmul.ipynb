{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fused softmax + matmul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory of the current notebook to sys.path\n",
    "cur_dir = Path().resolve()\n",
    "parent_dir = cur_dir.parent\n",
    "sys.path += [str(parent_dir), str(cur_dir)]\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "TEST_SIZES = [\n",
    "    (32, 16, 64),\n",
    "    (3, 16, 65),\n",
    "    (90, 2, 3),\n",
    "    (90, 2, 56),\n",
    "    (90, 2, 32),\n",
    "    (90, 2, 31),\n",
    "    (90, 2, 64),\n",
    "    (90, 2, 65),\n",
    "    (90, 32, 64),\n",
    "    (90, 32, 65),\n",
    "    (32, 16, 64),\n",
    "    (32, 16, 65),\n",
    "    (32, 2, 3),\n",
    "    (32, 2, 56),\n",
    "    (32, 2, 32),\n",
    "    (32, 2, 31),\n",
    "    (32, 2, 64),\n",
    "    (32, 2, 65),\n",
    "    (1024, 256, 56),\n",
    "    (1024, 256, 768),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import cdiv, get_sig, load_cuda, profile_kernel\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "def get_Q_K(sizes, Q_small, K_small, use_int):\n",
    "    N, L, M = sizes\n",
    "    if Q_small is None:\n",
    "        if use_int:\n",
    "            Q_small = torch.randint(low=10, high=20, size=(N, L)).contiguous().cuda().to(torch.float32)\n",
    "        else:\n",
    "            Q_small = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "    if K_small is None:\n",
    "        if use_int:\n",
    "            K_small = torch.randint(low=10, high=20, size=(L, M)).contiguous().cuda().to(torch.float32)\n",
    "        else:\n",
    "            K_small = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "    return Q_small, K_small\n",
    " \n",
    "def test_allclose(\n",
    "        kernels, \n",
    "        raise_error=True, \n",
    "        sizes=(32, 16, 64), \n",
    "        size_show=3, \n",
    "        Q_small=None, \n",
    "        K_small=None,\n",
    "        verbose=True,\n",
    "        atol=1e-4,\n",
    "        use_int=False,\n",
    "    ):\n",
    "   \n",
    "    Q_small, K_small = get_Q_K(sizes, Q_small, K_small, use_int)\n",
    "        \n",
    "    # run torch\n",
    "    O_torch = TorchNaiveSoftmaxAndMatMul().softmax_and_matmul(Q_small, K_small)\n",
    "    results = {\"torch\": O_torch}\n",
    "\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        if kernel_name != \"torch\":\n",
    "            module, fname = kernel_data[\"module\"], kernel_data[\"fname\"]\n",
    "            O = getattr(module, fname)(Q_small, K_small)\n",
    "            results[fname] = O\n",
    "            if not torch.allclose(O, O_torch, atol=atol):\n",
    "                if verbose:\n",
    "                    print(f\"{kernel_name=} failed:\\n\\n {O[:size_show, :size_show]=}\\n\\n {O_torch[:size_show, :size_show]=}\")\n",
    "                if raise_error:\n",
    "                    raise ValueError(f\"{kernel_name=} failed:\\n\\n {O[:size_show, :size_show]=}\\n\\n {O_torch[:size_show, :size_show]=}\")\n",
    "            if verbose:\n",
    "                print(f\"{kernel_name=} agrees with torch softmax\")\n",
    "    return results\n",
    "\n",
    "def test_is_normalised(\n",
    "        kernels, \n",
    "        raise_error=True, \n",
    "        sizes=(32, 16, 64), \n",
    "        size_show=3, \n",
    "        Q_small=None, \n",
    "        K_small=None,\n",
    "        verbose=True,\n",
    "        atol=1e-4,\n",
    "        use_int=False,\n",
    "    ):\n",
    "    Q_small, K_small = get_Q_K(sizes, Q_small, K_small, use_int)\n",
    "    \n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        if kernel_name != \"torch\":\n",
    "            module, fname = kernel_data[\"module\"], kernel_data[\"fname\"]\n",
    "            O = getattr(module, fname)(Q_small, K_small)\n",
    "            results[fname] = O\n",
    "            if not torch.allclose(O.sum(dim=1), torch.ones_like(O.sum(dim=1)), atol=atol):\n",
    "                if verbose:\n",
    "                    print(f\"{kernel_name=} rows do not sum up to 1:\\n\\n {O[:size_show, :size_show]=}\")\n",
    "                if raise_error:\n",
    "                    raise ValueError(f\"{kernel_name=}  rows do not sum up to 1:\\n\\n {O[:size_show, :size_show]=}\\n\\n {O_torch[:size_show, :size_show]=}\")\n",
    "            if verbose:\n",
    "                print(f\"{kernel_name=} rows sum up to 1\")\n",
    "    return results\n",
    "\n",
    "def profile_kernels(kernels, test_sizes=TEST_SIZES, test_kwargs={}):\n",
    "    for sizes in TEST_SIZES:\n",
    "        test_allclose(kernels, sizes=sizes, raise_error=True, **test_kwargs)\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        print(f\"Profiling: {kernel_name}\")\n",
    "        profile_kernel(kernel_data[\"module\"], kernel_data[\"fname\"], *kernel_data[\"args\"], **kernel_data[\"kwargs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python cuda looking implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchNaiveSoftmaxAndMatMul(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def softmax_and_matmul(self, Q, K):\n",
    "        O = Q@K\n",
    "        return torch.softmax(O, dim=1)\n",
    "\n",
    "# N, L, M = 32, 16, 64\n",
    "# Q_small = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "# K_small = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "# model = TorchNaiveSoftmaxAndMatMul()\n",
    "# output = model.softmax_and_matmul(Q_small, K_small)\n",
    "\n",
    "# print(\"Input Q shape:\", Q_small.shape)\n",
    "# print(\"Input K shape:\", K_small.shape)\n",
    "# print(\"Output shape :\", output.shape)\n",
    "# print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modules(kernels):\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "\n",
    "        fname = kernel_data[\"fname\"]\n",
    "        cuda_source = Path(kernel_data[\"cuda_source_path\"]).read_text()\n",
    "        cpp_source = get_sig(fname, cuda_source)\n",
    "        module = load_cuda(cuda_source, cpp_source, funcs=[fname])\n",
    "        kernel_data[\"module\"] = module\n",
    "\n",
    "\n",
    "def get_softmax_modules(kernels):\n",
    "    get_modules(kernels)\n",
    "    kernels[\"torch\"] = {\n",
    "        \"module\": TorchNaiveSoftmaxAndMatMul(),\n",
    "        \"fname\": \"softmax_and_matmul\",\n",
    "        \"kwargs\": {},\n",
    "    }\n",
    "\n",
    "def add_args_kwargs(kernels, *args, **kwargs):\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        kernel_data[\"args\"] = args\n",
    "        kernel_data[\"kwargs\"] = kwargs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/Workspace/cuda-examples/.venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "N, L, M = 1024, 256, 768 +1\n",
    "Q = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "K = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "\n",
    "kernels = {\n",
    "    \"fused_softmax_matmul\": dict(cuda_source_path = \"./fused_softmax_matmul.cu\", fname = \"fused_softmax_matmul\"),\n",
    "}\n",
    "get_softmax_modules(kernels)\n",
    "add_args_kwargs(kernels, Q, K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " tensor([[2., 3.],\n",
      "        [4., 5.]], device='cuda:0')\n",
      "K:\n",
      " tensor([[2., 3.],\n",
      "        [4., 5.]], device='cuda:0')\n",
      "O\n",
      "tensor([[6.6929e-03, 9.9331e-01],\n",
      "        [1.2339e-04, 9.9988e-01]], device='cuda:0')\n",
      "O_torch\n",
      "tensor([[6.6929e-03, 9.9331e-01],\n",
      "        [1.2339e-04, 9.9988e-01]], device='cuda:0')\n",
      "O sum\n",
      "tensor([1., 1.], device='cuda:0')\n",
      "O_torch sum\n",
      "tensor([1., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "module = kernels[\"fused_softmax_matmul\"][\"module\"]\n",
    "fname = kernels[\"fused_softmax_matmul\"][\"fname\"]\n",
    "Q = torch.tensor([[1, 2],\n",
    "                  [3, 4]], dtype=torch.float32).contiguous().cuda()\n",
    "\n",
    "K = torch.tensor([[1, 2],\n",
    "                  [3, 4]], dtype=torch.float32).contiguous().cuda()\n",
    "\n",
    "add_number = 1\n",
    "Q = Q + add_number * torch.sign(Q)\n",
    "\n",
    "K = K + add_number * torch.sign(K)\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"K:\\n\", K)\n",
    "\n",
    "O = getattr(module, fname)(Q, K)\n",
    "O_torch = TorchNaiveSoftmaxAndMatMul().softmax_and_matmul(Q, K)\n",
    "\n",
    "print(\"O\")\n",
    "print(O)\n",
    "print(\"O_torch\")\n",
    "print(O_torch)\n",
    "print(\"O sum\")\n",
    "print(O.sum(dim=1))\n",
    "print(\"O_torch sum\")\n",
    "print(O_torch.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "Profiling: fused_softmax_matmul\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "fused_softmax_matmul_kernel(float*, float*, float*, ...         0.00%       0.000us         0.00%       0.000us       0.000us       4.308ms        99.88%       4.308ms       4.308ms             1  \n",
      "                                            aten::zeros         0.55%      36.421us         2.81%     185.492us     185.492us       0.000us         0.00%       4.992us       4.992us             1  \n",
      "                                            aten::zero_         0.20%      13.190us         1.73%     114.581us     114.581us       0.000us         0.00%       4.992us       4.992us             1  \n",
      "                                            aten::fill_         0.40%      26.490us         1.53%     101.391us     101.391us       4.992us         0.12%       4.992us       4.992us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.992us         0.12%       4.992us       4.992us             1  \n",
      "                                  cudaDeviceSynchronize         0.47%      30.901us         0.47%      30.901us      10.300us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                           Unrecognized        31.16%       2.058ms        31.16%       2.058ms       2.058ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                            aten::empty         0.52%      34.490us         0.52%      34.490us      34.490us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel        66.70%       4.406ms        66.70%       4.406ms       2.203ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 6.606ms\n",
      "Self CUDA time total: 4.313ms\n",
      "\n",
      "Profiling: torch\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::matmul         0.36%       9.950us        34.36%     952.471us     952.471us       0.000us         0.00%      37.024us      37.024us             1  \n",
      "                                               aten::mm         3.75%     104.081us        34.00%     942.521us     942.521us      37.024us        67.94%      37.024us      37.024us             1  \n",
      "                                 ampere_sgemm_32x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us      37.024us        67.94%      37.024us      37.024us             1  \n",
      "                                          aten::softmax         0.19%       5.160us         2.18%      60.381us      60.381us       0.000us         0.00%      17.472us      17.472us             1  \n",
      "                                         aten::_softmax         0.75%      20.721us         1.99%      55.221us      55.221us      17.472us        32.06%      17.472us      17.472us             1  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us      17.472us        32.06%      17.472us      17.472us             1  \n",
      "                                  cudaDeviceSynchronize         0.84%      23.280us         0.84%      23.280us       7.760us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                           Unrecognized        62.63%       1.736ms        62.63%       1.736ms       1.736ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor        27.39%     759.399us        27.39%     759.399us     759.399us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel         4.10%     113.541us         4.10%     113.541us      56.771us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.772ms\n",
      "Self CUDA time total: 54.496us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_kernels(\n",
    "    kernels, \n",
    "    test_kwargs={\n",
    "        \"atol\":1e-0, # Trivialise tests as larger matrices have large approximation errors\n",
    "        \"use_int\": True,\n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1s matrices\n",
    "See that with matrices of one thoingd are fine for high tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0824, 0.0824, 0.0824],\n",
      "        [0.0824, 0.0824, 0.0824],\n",
      "        [0.0824, 0.0824, 0.0824],\n",
      "        [0.0824, 0.0824, 0.0824],\n",
      "        [0.0824, 0.0824, 0.0824]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0161, 0.0161, 0.0161, 0.0161, 0.0161],\n",
      "        [0.0161, 0.0161, 0.0161, 0.0161, 0.0161],\n",
      "        [0.0161, 0.0161, 0.0161, 0.0161, 0.0161],\n",
      "        [0.0161, 0.0161, 0.0161, 0.0161, 0.0161],\n",
      "        [0.0161, 0.0161, 0.0161, 0.0161, 0.0161]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
      "        [0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
      "        [0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
      "        [0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
      "        [0.0112, 0.0112, 0.0112, 0.0112, 0.0112]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0154, 0.0154, 0.0154, 0.0154, 0.0154],\n",
      "        [0.0154, 0.0154, 0.0154, 0.0154, 0.0154],\n",
      "        [0.0154, 0.0154, 0.0154, 0.0154, 0.0154],\n",
      "        [0.0154, 0.0154, 0.0154, 0.0154, 0.0154],\n",
      "        [0.0154, 0.0154, 0.0154, 0.0154, 0.0154]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0824, 0.0824, 0.0824],\n",
      "        [0.0824, 0.0824, 0.0824],\n",
      "        [0.0824, 0.0824, 0.0824],\n",
      "        [0.0824, 0.0824, 0.0824],\n",
      "        [0.0824, 0.0824, 0.0824]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0161, 0.0161, 0.0161, 0.0161, 0.0161],\n",
      "        [0.0161, 0.0161, 0.0161, 0.0161, 0.0161],\n",
      "        [0.0161, 0.0161, 0.0161, 0.0161, 0.0161],\n",
      "        [0.0161, 0.0161, 0.0161, 0.0161, 0.0161],\n",
      "        [0.0161, 0.0161, 0.0161, 0.0161, 0.0161]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
      "        [0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
      "        [0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
      "        [0.0112, 0.0112, 0.0112, 0.0112, 0.0112],\n",
      "        [0.0112, 0.0112, 0.0112, 0.0112, 0.0112]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0154, 0.0154, 0.0154, 0.0154, 0.0154],\n",
      "        [0.0154, 0.0154, 0.0154, 0.0154, 0.0154],\n",
      "        [0.0154, 0.0154, 0.0154, 0.0154, 0.0154],\n",
      "        [0.0154, 0.0154, 0.0154, 0.0154, 0.0154],\n",
      "        [0.0154, 0.0154, 0.0154, 0.0154, 0.0154]], device='cuda:0')\n",
      "{(3, 16, 65): True,\n",
      " (32, 2, 3): False,\n",
      " (32, 2, 31): True,\n",
      " (32, 2, 32): True,\n",
      " (32, 2, 56): False,\n",
      " (32, 2, 64): True,\n",
      " (32, 2, 65): False,\n",
      " (32, 16, 64): True,\n",
      " (32, 16, 65): True,\n",
      " (90, 2, 3): False,\n",
      " (90, 2, 31): True,\n",
      " (90, 2, 32): True,\n",
      " (90, 2, 56): False,\n",
      " (90, 2, 64): True,\n",
      " (90, 2, 65): False,\n",
      " (90, 32, 64): True,\n",
      " (90, 32, 65): True,\n",
      " (1024, 256, 56): True,\n",
      " (1024, 256, 768): True}\n"
     ]
    }
   ],
   "source": [
    "# Ones matrices\n",
    "results = {}\n",
    "for sizes in TEST_SIZES:\n",
    "    try:\n",
    "        N, L, M = sizes\n",
    "        test_allclose(kernels, raise_error=True, \n",
    "        size_show=5,\n",
    "        Q_small = torch.ones(N, L).contiguous().cuda(),\n",
    "        K_small = torch.ones(L, M).contiguous().cuda(),\n",
    "        verbose=False,\n",
    "        atol=1e-3,\n",
    "\n",
    "        )\n",
    "        results[sizes] = True\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        results[sizes] = False\n",
    "from pprint import  pprint\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test random matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[1.8746e-24, 1.1830e-20, 1.1598e-22],\n",
      "        [1.4867e-25, 5.8047e-20, 8.5556e-25],\n",
      "        [5.9265e-23, 1.2798e-20, 5.5125e-24]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[5.8067e-04, 6.4352e-03, 5.1341e-13],\n",
      "        [1.5850e-04, 3.3223e-02, 7.4096e-15],\n",
      "        [1.4823e-03, 1.1002e-02, 4.2304e-11]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[1.4146e-21, 9.2884e-14, 0.0000e+00],\n",
      "        [4.1450e-25, 9.7566e-08, 0.0000e+00],\n",
      "        [4.1638e-18, 1.3612e-11, 0.0000e+00]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[3.5035e-13, 8.4036e-11, 1.2350e-17],\n",
      "        [3.9179e-11, 6.5400e-07, 4.7177e-13],\n",
      "        [8.2501e-12, 1.3833e-08, 7.0070e-15]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.8049e-35, 0.0000e+00],\n",
      "        [0.0000e+00, 5.6052e-45, 0.0000e+00]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[2.9820e-18, 6.0065e-13, 2.1417e-06],\n",
      "        [7.1026e-19, 3.5118e-13, 2.1462e-06],\n",
      "        [9.2163e-16, 5.1125e-12, 2.1123e-06]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0000e+00, 0.0000e+00, 5.3802e-32],\n",
      "        [0.0000e+00, 0.0000e+00, 5.3802e-32],\n",
      "        [0.0000e+00, 0.0000e+00, 5.3802e-32]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[1.1348e-05, 7.2546e-07, 1.8710e-05],\n",
      "        [1.8027e-06, 2.0026e-08, 6.6318e-07],\n",
      "        [4.0442e-05, 2.5854e-06, 4.0442e-05]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[6.0546e-39, 0.0000e+00, 3.3057e-37],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.6251e-34, 1.0089e-43, 3.6251e-34]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[7.4928e-07, 5.6638e-03, 9.2426e-10],\n",
      "        [7.4928e-07, 5.6638e-03, 9.2426e-10],\n",
      "        [1.9834e-06, 1.1699e-02, 2.2813e-08]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0000e+00, 1.1598e-16, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1598e-16, 0.0000e+00],\n",
      "        [1.9618e-44, 4.6788e-14, 0.0000e+00]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[8.3335e-13, 3.8825e-27, 7.3102e-01],\n",
      "        [3.9893e-20, 8.2226e-29, 9.3903e-03],\n",
      "        [5.8430e-21, 9.0771e-27, 9.9631e-05]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0000e+00, 0.0000e+00, 9.9966e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 8.5330e-17],\n",
      "        [0.0000e+00, 0.0000e+00, 1.9793e-32]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[3.4557e-39, 1.7608e-40, 2.2708e-33],\n",
      "        [1.3018e-41, 1.3957e-42, 1.4830e-34],\n",
      "        [2.2339e-31, 1.5057e-36, 1.4131e-28]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[3.1293e-13, 4.4946e-08, 2.1507e-13],\n",
      "        [1.4660e-10, 1.3909e-09, 2.7938e-08],\n",
      "        [5.3847e-13, 2.5109e-08, 5.6754e-14]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[9.4692e-09, 1.0000e+00, 1.4519e-30],\n",
      "        [1.7216e-05, 9.9998e-01, 4.4831e-34],\n",
      "        [1.7099e-06, 1.0000e+00, 8.2065e-30]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[1.2664e-14, 1.0000e+00, 0.0000e+00],\n",
      "        [5.6028e-09, 1.0000e+00, 0.0000e+00],\n",
      "        [1.0262e-10, 1.0000e+00, 0.0000e+00]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[2.4894e-06, 1.8477e-05, 4.1615e-01],\n",
      "        [6.1194e-05, 2.6613e-04, 4.1402e-01],\n",
      "        [2.5257e-06, 1.8746e-05, 4.2222e-01]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[4.0969e-40, 1.3393e-33, 4.9999e-01],\n",
      "        [1.0853e-29, 6.4978e-25, 5.0000e-01],\n",
      "        [4.0970e-40, 1.3393e-33, 5.0000e-01]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[7.2694e-11, 1.4481e-06, 2.9633e-08],\n",
      "        [3.6431e-13, 1.4718e-08, 8.6604e-09],\n",
      "        [2.1275e-12, 7.1702e-07, 2.1085e-10]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0000e+00, 9.8542e-34, 2.7465e-43],\n",
      "        [0.0000e+00, 5.6052e-45, 0.0000e+00],\n",
      "        [0.0000e+00, 1.8049e-35, 0.0000e+00]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[8.5094e-11, 1.0518e-09, 2.0596e-16],\n",
      "        [4.6065e-12, 8.6984e-09, 3.2754e-17],\n",
      "        [1.0505e-07, 2.4174e-09, 6.2414e-13]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 5.6052e-45, 0.0000e+00],\n",
      "        [6.0545e-39, 0.0000e+00, 0.0000e+00]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[1.0959e-05, 2.2298e-09, 6.3063e-05],\n",
      "        [1.9650e-04, 2.6940e-10, 2.8591e-04],\n",
      "        [2.1744e-05, 1.0826e-06, 2.0630e-04]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[8.1940e-40, 0.0000e+00, 9.8542e-34],\n",
      "        [2.1705e-29, 0.0000e+00, 4.3595e-28],\n",
      "        [3.3057e-37, 0.0000e+00, 2.1705e-29]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[3.5467e-13, 1.0003e-02, 5.2615e-08],\n",
      "        [1.6087e-12, 1.6821e-02, 2.6592e-05],\n",
      "        [3.5048e-13, 8.7318e-03, 1.7027e-08]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0000e+00, 1.1155e-14, 0.0000e+00],\n",
      "        [0.0000e+00, 6.9120e-13, 1.8042e-35],\n",
      "        [0.0000e+00, 1.6831e-15, 0.0000e+00]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[2.7408e-37, 0.0000e+00, 0.0000e+00],\n",
      "        [2.7410e-37, 0.0000e+00, 0.0000e+00],\n",
      "        [1.7256e-35, 0.0000e+00, 0.0000e+00]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[9.3564e-12, 1.4398e-06, 2.8833e-10],\n",
      "        [8.8895e-11, 2.5835e-07, 5.6369e-09],\n",
      "        [8.1437e-12, 1.2618e-05, 1.0182e-10]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "{(3, 16, 65): True,\n",
      " (32, 2, 3): False,\n",
      " (32, 2, 31): False,\n",
      " (32, 2, 32): False,\n",
      " (32, 2, 56): False,\n",
      " (32, 2, 64): False,\n",
      " (32, 2, 65): False,\n",
      " (32, 16, 64): False,\n",
      " (32, 16, 65): True,\n",
      " (90, 2, 3): True,\n",
      " (90, 2, 31): False,\n",
      " (90, 2, 32): False,\n",
      " (90, 2, 56): False,\n",
      " (90, 2, 64): False,\n",
      " (90, 2, 65): False,\n",
      " (90, 32, 64): False,\n",
      " (90, 32, 65): False,\n",
      " (1024, 256, 56): False,\n",
      " (1024, 256, 768): False}\n"
     ]
    }
   ],
   "source": [
    "#  Random matrices\n",
    "results = {}\n",
    "for sizes in TEST_SIZES:\n",
    "    try:\n",
    "        N, L, M = sizes\n",
    "        test_allclose(\n",
    "            kernels, \n",
    "            raise_error=True, \n",
    "            size_show=3,\n",
    "            sizes=sizes,\n",
    "            verbose=False,\n",
    "            use_int=True,\n",
    "            atol=1e-2,\n",
    "        )\n",
    "        results[sizes] = True\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        results[sizes] = False\n",
    "from pprint import  pprint\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test random matrices row are normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fused_softmax_matmul': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
      " (3, 16, 65): True,\n",
      " (32, 2, 3): True,\n",
      " (32, 2, 31): True,\n",
      " (32, 2, 32): True,\n",
      " (32, 2, 56): True,\n",
      " (32, 2, 64): True,\n",
      " (32, 2, 65): True,\n",
      " (32, 16, 64): True,\n",
      " (32, 16, 65): True,\n",
      " (90, 2, 3): True,\n",
      " (90, 2, 31): True,\n",
      " (90, 2, 32): True,\n",
      " (90, 2, 56): True,\n",
      " (90, 2, 64): True,\n",
      " (90, 2, 65): True,\n",
      " (90, 32, 64): True,\n",
      " (90, 32, 65): True,\n",
      " (1024, 256, 56): True,\n",
      " (1024, 256, 768): True}\n"
     ]
    }
   ],
   "source": [
    "#  Random matrices are normalised\n",
    "results = {}\n",
    "for sizes in TEST_SIZES:\n",
    "    try:\n",
    "        N, L, M = sizes\n",
    "        test_is_normalised(\n",
    "            kernels, \n",
    "            raise_error=True, \n",
    "            size_show=3,\n",
    "            sizes=sizes,\n",
    "            verbose=False,\n",
    "            use_int=True, # Many fails if False\n",
    "        )\n",
    "        results[sizes] = True\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        results[sizes] = False\n",
    "from pprint import  pprint\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Ones matrices\n",
    "sizes = 1024, 256, 56\n",
    "N, L, M = sizes\n",
    "results = test_allclose(kernels, raise_error=False, \n",
    "# sizes=(32, 16, 64), \n",
    "# sizes=(90, 2, 3), \n",
    "# sizes=(1024, 256, 768), \n",
    "size_show=None,\n",
    "Q_small = torch.ones(N, L).contiguous().cuda(),\n",
    "K_small = torch.ones(L, M).contiguous().cuda(),\n",
    ")\n",
    "O, O_torch = results[\"fused_softmax_matmul\"], results[\"torch\"] \n",
    "        \n",
    "O_torch\n",
    "O_torch.sum(dim=1)[:5]\n",
    "O.sum(dim=1)[:5]\n",
    "torch.allclose(O, O_torch, atol=1e-4)\n",
    "O.sum(dim=1)\n",
    "O[:3, :3]\n",
    "O_torch[:3, :3]\n",
    "test_allclose(kernels, \n",
    "sizes=(32, 16, 64),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
