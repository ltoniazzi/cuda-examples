{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fused softmax + matmul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory of the current notebook to sys.path\n",
    "cur_dir = Path().resolve()\n",
    "parent_dir = cur_dir.parent\n",
    "sys.path += [str(parent_dir), str(cur_dir)]\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import cdiv, get_sig, load_cuda, profile_kernel\n",
    "from collections import namedtuple\n",
    "\n",
    "def test_allclose(kernels, sizes=(32, 16, 64)):\n",
    "    N, L, M = sizes\n",
    "    Q_small = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "    K_small = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "    O_torch = TorchNaiveSoftmaxAndMatMul().softmax_and_matmul(Q_small, K_small)\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        if kernel_name!=\"torch\":\n",
    "            module, fname = kernel_data[\"module\"], kernel_data[\"fname\"]\n",
    "            O = getattr(module, fname)(Q_small, K_small)\n",
    "            if not torch.allclose(O, O_torch, atol=1e-4):\n",
    "                SIZE = 3\n",
    "                raise ValueError(f\"{kernel_name=} failed:\\n\\n {O[:SIZE, :SIZE]=}\\n\\n, {O_torch[:SIZE, :SIZE]=}\")\n",
    "            print(f\"{kernel_name=} agrees with torch softmax\")\n",
    "        \n",
    "\n",
    "\n",
    "def profile_kernels(kernels):\n",
    "    test_allclose(kernels)\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        print(f\"Profiling: {kernel_name}\")\n",
    "        profile_kernel(kernel_data[\"module\"], kernel_data[\"fname\"], *kernel_data[\"args\"], **kernel_data[\"kwargs\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python cuda looking implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchNaiveSoftmaxAndMatMul(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def softmax_and_matmul(self, Q, K):\n",
    "        O = Q@K\n",
    "        return torch.softmax(O, dim=1)\n",
    "\n",
    "# N, L, M = 32, 16, 64\n",
    "# Q_small = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "# K_small = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "# model = TorchNaiveSoftmaxAndMatMul()\n",
    "# output = model.softmax_and_matmul(Q_small, K_small)\n",
    "\n",
    "# print(\"Input Q shape:\", Q_small.shape)\n",
    "# print(\"Input K shape:\", K_small.shape)\n",
    "# print(\"Output shape :\", output.shape)\n",
    "# print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modules(kernels):\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "\n",
    "        fname = kernel_data[\"fname\"]\n",
    "        cuda_source = Path(kernel_data[\"cuda_source_path\"]).read_text()\n",
    "        cpp_source = get_sig(fname, cuda_source)\n",
    "        module = load_cuda(cuda_source, cpp_source, funcs=[fname])\n",
    "        kernel_data[\"module\"] = module\n",
    "\n",
    "\n",
    "def get_softmax_modules(kernels):\n",
    "    get_modules(kernels)\n",
    "    kernels[\"torch\"] = {\n",
    "        \"module\": TorchNaiveSoftmaxAndMatMul(),\n",
    "        \"fname\": \"softmax_and_matmul\",\n",
    "        \"kwargs\": {},\n",
    "    }\n",
    "\n",
    "def add_args_kwargs(kernels, *args, **kwargs):\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        kernel_data[\"args\"] = args\n",
    "        kernel_data[\"kwargs\"] = kwargs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/Workspace/cuda-examples/.venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "N, L, M = 1024, 256, 768 +1\n",
    "Q = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "K = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "\n",
    "kernels = {\n",
    "    \"fused_softmax_matmul\": dict(cuda_source_path = \"./fused_softmax_matmul.cu\", fname = \"fused_softmax_matmul\"),\n",
    "}\n",
    "get_softmax_modules(kernels)\n",
    "add_args_kwargs(kernels, Q, K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "Profiling: fused_softmax_matmul\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "fused_softmax_matmul_kernel(float*, float*, float*, ...         0.00%       0.000us         0.00%       0.000us       0.000us       4.307ms        99.90%       4.307ms       4.307ms             1  \n",
      "                                            aten::zeros         0.66%      39.750us         2.38%     143.801us     143.801us       0.000us         0.00%       4.384us       4.384us             1  \n",
      "                                            aten::zero_         0.15%       9.331us         1.27%      76.851us      76.851us       0.000us         0.00%       4.384us       4.384us             1  \n",
      "                                            aten::fill_         0.36%      21.740us         1.12%      67.520us      67.520us       4.384us         0.10%       4.384us       4.384us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       4.384us         0.10%       4.384us       4.384us             1  \n",
      "                                  cudaDeviceSynchronize         0.27%      16.510us         0.27%      16.510us       5.503us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                           Unrecognized        26.01%       1.575ms        26.01%       1.575ms       1.575ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                            aten::empty         0.45%      27.200us         0.45%      27.200us      27.200us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel        72.10%       4.365ms        72.10%       4.365ms       2.182ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 6.054ms\n",
      "Self CUDA time total: 4.311ms\n",
      "\n",
      "Profiling: torch\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::matmul         0.48%       9.280us        11.05%     212.923us     212.923us       0.000us         0.00%      36.897us      36.897us             1  \n",
      "                                               aten::mm         6.30%     121.372us        10.57%     203.643us     203.643us      36.897us        68.96%      36.897us      36.897us             1  \n",
      "                                 ampere_sgemm_32x128_nn         0.00%       0.000us         0.00%       0.000us       0.000us      36.897us        68.96%      36.897us      36.897us             1  \n",
      "                                          aten::softmax         0.27%       5.160us         5.89%     113.482us     113.482us       0.000us         0.00%      33.216us      33.216us             1  \n",
      "                                         aten::_softmax         0.96%      18.570us         5.62%     108.322us     108.322us      16.608us        31.04%      33.216us      33.216us             1  \n",
      "                                       cudaLaunchKernel         6.10%     117.573us         8.65%     166.603us      83.301us       0.000us         0.00%      16.608us       8.304us             2  \n",
      "                                  Lazy Function Loading         2.55%      49.030us         2.55%      49.030us      49.030us      16.608us        31.04%      16.608us      16.608us             1  \n",
      "void (anonymous namespace)::softmax_warp_forward<flo...         0.00%       0.000us         0.00%       0.000us       0.000us      16.608us        31.04%      16.608us      16.608us             1  \n",
      "                                  cudaDeviceSynchronize         1.21%      23.400us         1.21%      23.400us       7.800us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                           Unrecognized        81.84%       1.577ms        81.84%       1.577ms       1.577ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "          cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.28%       5.420us         0.28%       5.420us       5.420us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.926ms\n",
      "Self CUDA time total: 53.505us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_kernels(kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n"
     ]
    }
   ],
   "source": [
    "def test_allclose_new(kernels, not_raise=False, sizes=(32, 16, 64), size_show=3, Q_small=None, K_small=None):\n",
    "    N, L, M = sizes\n",
    "    if Q_small is None:\n",
    "        Q_small = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "    if K_small is None:\n",
    "        K_small = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "    O_torch = TorchNaiveSoftmaxAndMatMul().softmax_and_matmul(Q_small, K_small)\n",
    "    results = {\"torch\": O_torch}\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        if kernel_name!=\"torch\":\n",
    "            module, fname = kernel_data[\"module\"], kernel_data[\"fname\"]\n",
    "            O = getattr(module, fname)(Q_small, K_small)\n",
    "            results[fname] = O\n",
    "            if not torch.allclose(O, O_torch, atol=1e-4):\n",
    "                if not_raise:\n",
    "                    print(f\"{kernel_name=} failed:\\n\\n {O[:size_show, :size_show]=}\\n\\n {O_torch[:size_show, :size_show]=}\")\n",
    "                else:\n",
    "                    ValueError(f\"{kernel_name=} failed:\\n\\n {O[:size_show, :size_show]=}\\n\\n {O_torch[:size_show, :size_show]=}\")\n",
    "\n",
    "            print(f\"{kernel_name=} agrees with torch softmax\")\n",
    "    return results\n",
    "\n",
    "# Ones matrices\n",
    "sizes = 1024, 256, 56\n",
    "N, L, M = sizes\n",
    "results = test_allclose_new(kernels, not_raise=True, \n",
    "# sizes=(32, 16, 64), \n",
    "# sizes=(90, 2, 3), \n",
    "# sizes=(1024, 256, 768), \n",
    "size_show=None,\n",
    "Q_small = torch.ones(N, L).contiguous().cuda(),\n",
    "K_small = torch.ones(L, M).contiguous().cuda(),\n",
    ")\n",
    "O, O_torch = results[\"fused_softmax_matmul\"], results[\"torch\"] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179],\n",
       "        ...,\n",
       "        [0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_torch.sum(dim=1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O.sum(dim=1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(O, O_torch, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_torch[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n"
     ]
    }
   ],
   "source": [
    "test_allclose(kernels, \n",
    "sizes=(32, 16, 64),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEst 1s matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "{(3, 16, 65): True,\n",
      " (32, 16, 64): True,\n",
      " (32, 16, 65): True,\n",
      " (90, 2, 3): True,\n",
      " (90, 2, 31): True,\n",
      " (90, 2, 32): True,\n",
      " (90, 2, 56): True,\n",
      " (1024, 256, 56): True,\n",
      " (1024, 256, 768): True}\n"
     ]
    }
   ],
   "source": [
    "# Ones matrices\n",
    "sizes_list = [\n",
    "    (1024, 256, 56),\n",
    "    (90, 2, 56),\n",
    "    (90, 2, 32),\n",
    "    (90, 2, 31),\n",
    "    (90, 2, 3),\n",
    "    (1024, 256, 768),\n",
    "    (32, 16, 64),\n",
    "    (32, 16, 65),\n",
    "    (3, 16, 65),\n",
    "]\n",
    "results = {}\n",
    "for sizes in sizes_list:\n",
    "    try:\n",
    "        N, L, M = sizes\n",
    "        test_allclose_new(kernels, not_raise=False, \n",
    "        size_show=None,\n",
    "        Q_small = torch.ones(N, L).contiguous().cuda(),\n",
    "        K_small = torch.ones(L, M).contiguous().cuda(),\n",
    "        )\n",
    "        results[sizes] = True\n",
    "    except:\n",
    "        results[sizes] = False\n",
    "from pprint import  pprint\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "{(3, 16, 65): True,\n",
      " (32, 16, 64): True,\n",
      " (32, 16, 65): True,\n",
      " (90, 2, 3): True,\n",
      " (90, 2, 31): True,\n",
      " (90, 2, 32): True,\n",
      " (90, 2, 56): True,\n",
      " (1024, 256, 56): True,\n",
      " (1024, 256, 768): True}\n"
     ]
    }
   ],
   "source": [
    "#  Random matrices\n",
    "sizes_list = [\n",
    "    (1024, 256, 56),\n",
    "    (90, 2, 56),\n",
    "    (90, 2, 32),\n",
    "    (90, 2, 31),\n",
    "    (90, 2, 3),\n",
    "    (1024, 256, 768),\n",
    "    (32, 16, 64),\n",
    "    (32, 16, 65),\n",
    "    (3, 16, 65),\n",
    "]\n",
    "results = {}\n",
    "for sizes in sizes_list:\n",
    "    try:\n",
    "        N, L, M = sizes\n",
    "        test_allclose_new(\n",
    "            kernels, \n",
    "            not_raise=False, \n",
    "            size_show=3,\n",
    "            sizes=sizes,\n",
    "        )\n",
    "        results[sizes] = True\n",
    "    except:\n",
    "        results[sizes] = False\n",
    "from pprint import  pprint\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
