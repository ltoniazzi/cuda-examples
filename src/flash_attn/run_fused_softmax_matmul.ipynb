{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fused softmax + matmul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory of the current notebook to sys.path\n",
    "cur_dir = Path().resolve()\n",
    "parent_dir = cur_dir.parent\n",
    "sys.path += [str(parent_dir), str(cur_dir)]\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "TEST_SIZES = [\n",
    "    (32, 16, 64),\n",
    "    (3, 16, 65),\n",
    "    (90, 2, 3),\n",
    "    (90, 2, 56),\n",
    "    (90, 2, 32),\n",
    "    (90, 2, 31),\n",
    "    (90, 2, 64),\n",
    "    (90, 2, 65),\n",
    "    (90, 32, 64),\n",
    "    (90, 32, 65),\n",
    "    (32, 16, 64),\n",
    "    (32, 16, 65),\n",
    "    (32, 2, 3),\n",
    "    (32, 2, 56),\n",
    "    (32, 2, 32),\n",
    "    (32, 2, 31),\n",
    "    (32, 2, 64),\n",
    "    (32, 2, 65),\n",
    "    (1024, 256, 56),\n",
    "    (1024, 256, 768),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import cdiv, get_sig, load_cuda, profile_kernel\n",
    "from collections import namedtuple\n",
    "\n",
    "# def test_allclose_old(kernels, sizes=(32, 16, 64)):\n",
    "#     N, L, M = sizes\n",
    "#     Q_small = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "#     K_small = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "#     O_torch = TorchNaiveSoftmaxAndMatMul().softmax_and_matmul(Q_small, K_small)\n",
    "#     for kernel_name, kernel_data in kernels.items():\n",
    "#         if kernel_name!=\"torch\":\n",
    "#             module, fname = kernel_data[\"module\"], kernel_data[\"fname\"]\n",
    "#             O = getattr(module, fname)(Q_small, K_small)\n",
    "#             if not torch.allclose(O, O_torch, atol=1e-4):\n",
    "#                 SIZE = 3\n",
    "#                 raise ValueError(f\"{kernel_name=} failed:\\n\\n {O[:SIZE, :SIZE]=}\\n\\n, {O_torch[:SIZE, :SIZE]=}\")\n",
    "#             print(f\"{kernel_name=} agrees with torch softmax\")\n",
    "\n",
    "\n",
    "def get_Q_K(sizes, Q_small, K_small, use_int):\n",
    "    N, L, M = sizes\n",
    "    if Q_small is None:\n",
    "        if use_int:\n",
    "            Q_small = torch.randint(low=10, high=20, size=(N, L)).contiguous().cuda().to(torch.float32)\n",
    "        else:\n",
    "            Q_small = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "    if K_small is None:\n",
    "        if use_int:\n",
    "            K_small = torch.randint(low=10, high=20, size=(L, M)).contiguous().cuda().to(torch.float32)\n",
    "        else:\n",
    "            K_small = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "    return Q_small, K_small\n",
    " \n",
    "def test_allclose(\n",
    "        kernels, \n",
    "        raise_error=True, \n",
    "        sizes=(32, 16, 64), \n",
    "        size_show=3, \n",
    "        Q_small=None, \n",
    "        K_small=None,\n",
    "        verbose=True,\n",
    "        atol=1e-4,\n",
    "        use_int=False,\n",
    "    ):\n",
    "   \n",
    "    Q_small, K_small = get_Q_K(sizes, Q_small, K_small, use_int)\n",
    "        \n",
    "    # run torch\n",
    "    O_torch = TorchNaiveSoftmaxAndMatMul().softmax_and_matmul(Q_small, K_small)\n",
    "    results = {\"torch\": O_torch}\n",
    "\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        if kernel_name != \"torch\":\n",
    "            module, fname = kernel_data[\"module\"], kernel_data[\"fname\"]\n",
    "            O = getattr(module, fname)(Q_small, K_small)\n",
    "            results[fname] = O\n",
    "            if not torch.allclose(O, O_torch, atol=atol):\n",
    "                if verbose:\n",
    "                    print(f\"{kernel_name=} failed:\\n\\n {O[:size_show, :size_show]=}\\n\\n {O_torch[:size_show, :size_show]=}\")\n",
    "                if raise_error:\n",
    "                    raise ValueError(f\"{kernel_name=} failed:\\n\\n {O[:size_show, :size_show]=}\\n\\n {O_torch[:size_show, :size_show]=}\")\n",
    "            if verbose:\n",
    "                print(f\"{kernel_name=} agrees with torch softmax\")\n",
    "    return results\n",
    "\n",
    "def test_is_normalised(\n",
    "        kernels, \n",
    "        raise_error=True, \n",
    "        sizes=(32, 16, 64), \n",
    "        size_show=3, \n",
    "        Q_small=None, \n",
    "        K_small=None,\n",
    "        verbose=True,\n",
    "        atol=1e-4,\n",
    "        use_int=False,\n",
    "    ):\n",
    "    Q_small, K_small = get_Q_K(sizes, Q_small, K_small, use_int)\n",
    "    \n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        if kernel_name != \"torch\":\n",
    "            module, fname = kernel_data[\"module\"], kernel_data[\"fname\"]\n",
    "            O = getattr(module, fname)(Q_small, K_small)\n",
    "            results[fname] = O\n",
    "            if not torch.allclose(O.sum(dim=1), torch.ones_like(O.sum(dim=1)), atol=atol):\n",
    "                if verbose:\n",
    "                    print(f\"{kernel_name=} rows do not sum up to 1:\\n\\n {O[:size_show, :size_show]=}\")\n",
    "                if raise_error:\n",
    "                    raise ValueError(f\"{kernel_name=}  rows do not sum up to 1:\\n\\n {O[:size_show, :size_show]=}\\n\\n {O_torch[:size_show, :size_show]=}\")\n",
    "            if verbose:\n",
    "                print(f\"{kernel_name=} rows sum up to 1\")\n",
    "    return results\n",
    "\n",
    "def profile_kernels(kernels, test_sizes=TEST_SIZES):\n",
    "    for sizes in TEST_SIZES:\n",
    "        test_allclose(kernels, sizes=sizes, raise_error=True)\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        print(f\"Profiling: {kernel_name}\")\n",
    "        profile_kernel(kernel_data[\"module\"], kernel_data[\"fname\"], *kernel_data[\"args\"], **kernel_data[\"kwargs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python cuda looking implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TorchNaiveSoftmaxAndMatMul(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def softmax_and_matmul(self, Q, K):\n",
    "        O = Q@K\n",
    "        return torch.softmax(O, dim=1)\n",
    "\n",
    "# N, L, M = 32, 16, 64\n",
    "# Q_small = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "# K_small = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "# model = TorchNaiveSoftmaxAndMatMul()\n",
    "# output = model.softmax_and_matmul(Q_small, K_small)\n",
    "\n",
    "# print(\"Input Q shape:\", Q_small.shape)\n",
    "# print(\"Input K shape:\", K_small.shape)\n",
    "# print(\"Output shape :\", output.shape)\n",
    "# print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modules(kernels):\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "\n",
    "        fname = kernel_data[\"fname\"]\n",
    "        cuda_source = Path(kernel_data[\"cuda_source_path\"]).read_text()\n",
    "        cpp_source = get_sig(fname, cuda_source)\n",
    "        module = load_cuda(cuda_source, cpp_source, funcs=[fname])\n",
    "        kernel_data[\"module\"] = module\n",
    "\n",
    "\n",
    "def get_softmax_modules(kernels):\n",
    "    get_modules(kernels)\n",
    "    kernels[\"torch\"] = {\n",
    "        \"module\": TorchNaiveSoftmaxAndMatMul(),\n",
    "        \"fname\": \"softmax_and_matmul\",\n",
    "        \"kwargs\": {},\n",
    "    }\n",
    "\n",
    "def add_args_kwargs(kernels, *args, **kwargs):\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        kernel_data[\"args\"] = args\n",
    "        kernel_data[\"kwargs\"] = kwargs\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/Workspace/cuda-examples/.venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "N, L, M = 1024, 256, 768 +1\n",
    "Q = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "K = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "\n",
    "kernels = {\n",
    "    \"fused_softmax_matmul\": dict(cuda_source_path = \"./fused_softmax_matmul.cu\", fname = \"fused_softmax_matmul\"),\n",
    "}\n",
    "get_softmax_modules(kernels)\n",
    "add_args_kwargs(kernels, Q, K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " tensor([[2., 3.],\n",
      "        [4., 5.]], device='cuda:0')\n",
      "K:\n",
      " tensor([[2., 3.],\n",
      "        [4., 5.]], device='cuda:0')\n",
      "O\n",
      "tensor([[6.6929e-03, 9.9331e-01],\n",
      "        [1.2339e-04, 9.9988e-01]], device='cuda:0')\n",
      "O_torch\n",
      "tensor([[6.6929e-03, 9.9331e-01],\n",
      "        [1.2339e-04, 9.9988e-01]], device='cuda:0')\n",
      "O sum\n",
      "tensor([1., 1.], device='cuda:0')\n",
      "O_torch sum\n",
      "tensor([1., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "module = kernels[\"fused_softmax_matmul\"][\"module\"]\n",
    "fname = kernels[\"fused_softmax_matmul\"][\"fname\"]\n",
    "Q = torch.tensor([[1, 2],\n",
    "                  [3, 4]], dtype=torch.float32).contiguous().cuda()\n",
    "\n",
    "K = torch.tensor([[1, 2],\n",
    "                  [3, 4]], dtype=torch.float32).contiguous().cuda()\n",
    "\n",
    "add_number = 1\n",
    "Q = Q + add_number * torch.sign(Q)\n",
    "\n",
    "K = K + add_number * torch.sign(K)\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"K:\\n\", K)\n",
    "\n",
    "O = getattr(module, fname)(Q, K)\n",
    "O_torch = TorchNaiveSoftmaxAndMatMul().softmax_and_matmul(Q, K)\n",
    "\n",
    "print(\"O\")\n",
    "print(O)\n",
    "print(\"O_torch\")\n",
    "print(O_torch)\n",
    "print(\"O sum\")\n",
    "print(O.sum(dim=1))\n",
    "print(\"O_torch sum\")\n",
    "print(O_torch.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[7.6237e-08, 5.1181e-07, 2.3512e-06],\n",
      "        [2.2461e-03, 4.7643e-05, 1.4328e-05],\n",
      "        [5.5389e-08, 9.9053e-02, 1.5366e-08]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[7.6252e-08, 5.1191e-07, 2.3516e-06],\n",
      "        [2.2581e-03, 4.7899e-05, 1.4405e-05],\n",
      "        [5.5434e-08, 9.9132e-02, 1.5379e-08]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "kernel_name='fused_softmax_matmul' failed:\n\n O[:size_show, :size_show]=tensor([[7.6237e-08, 5.1181e-07, 2.3512e-06],\n        [2.2461e-03, 4.7643e-05, 1.4328e-05],\n        [5.5389e-08, 9.9053e-02, 1.5366e-08]], device='cuda:0')\n\n O_torch[:size_show, :size_show]=tensor([[7.6252e-08, 5.1191e-07, 2.3516e-06],\n        [2.2581e-03, 4.7899e-05, 1.4405e-05],\n        [5.5434e-08, 9.9132e-02, 1.5379e-08]], device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprofile_kernels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mprofile_kernels\u001b[39m\u001b[34m(kernels, test_sizes)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprofile_kernels\u001b[39m(kernels, test_sizes=TEST_SIZES):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m sizes \u001b[38;5;129;01min\u001b[39;00m TEST_SIZES:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         \u001b[43mtest_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m kernel_name, kernel_data \u001b[38;5;129;01min\u001b[39;00m kernels.items():\n\u001b[32m     97\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProfiling: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mtest_allclose\u001b[39m\u001b[34m(kernels, raise_error, sizes, size_show, Q_small, K_small, verbose, atol, use_int)\u001b[39m\n\u001b[32m     59\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_name\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m failed:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mO[:size_show,\u001b[38;5;250m \u001b[39m:size_show]\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mO_torch[:size_show,\u001b[38;5;250m \u001b[39m:size_show]\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_name\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m failed:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mO[:size_show,\u001b[38;5;250m \u001b[39m:size_show]\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mO_torch[:size_show,\u001b[38;5;250m \u001b[39m:size_show]\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_name\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m agrees with torch softmax\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: kernel_name='fused_softmax_matmul' failed:\n\n O[:size_show, :size_show]=tensor([[7.6237e-08, 5.1181e-07, 2.3512e-06],\n        [2.2461e-03, 4.7643e-05, 1.4328e-05],\n        [5.5389e-08, 9.9053e-02, 1.5366e-08]], device='cuda:0')\n\n O_torch[:size_show, :size_show]=tensor([[7.6252e-08, 5.1191e-07, 2.3516e-06],\n        [2.2581e-03, 4.7899e-05, 1.4405e-05],\n        [5.5434e-08, 9.9132e-02, 1.5379e-08]], device='cuda:0')"
     ]
    }
   ],
   "source": [
    "profile_kernels(kernels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ones matrices\n",
    "sizes = 1024, 256, 56\n",
    "N, L, M = sizes\n",
    "results = test_allclose(kernels, raise_error=False, \n",
    "# sizes=(32, 16, 64), \n",
    "# sizes=(90, 2, 3), \n",
    "# sizes=(1024, 256, 768), \n",
    "size_show=None,\n",
    "Q_small = torch.ones(N, L).contiguous().cuda(),\n",
    "K_small = torch.ones(L, M).contiguous().cuda(),\n",
    ")\n",
    "O, O_torch = results[\"fused_softmax_matmul\"], results[\"torch\"] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179],\n",
       "        ...,\n",
       "        [0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179,  ..., 0.0179, 0.0179, 0.0179]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_torch.sum(dim=1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O.sum(dim=1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(O, O_torch, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179]], device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179],\n",
       "        [0.0179, 0.0179, 0.0179]], device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_torch[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' agrees with torch softmax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'torch': tensor([[8.7599e-08, 1.1568e-04, 8.5495e-05,  ..., 3.2460e-10, 3.2980e-08,\n",
       "          4.0416e-11],\n",
       "         [1.2988e-03, 8.0773e-06, 1.1501e-01,  ..., 7.0932e-04, 1.0986e-04,\n",
       "          9.1397e-04],\n",
       "         [6.8656e-04, 1.0944e-05, 1.6643e-04,  ..., 5.3404e-05, 6.4530e-06,\n",
       "          1.3815e-06],\n",
       "         ...,\n",
       "         [5.9146e-07, 3.2172e-05, 1.4822e-05,  ..., 2.2011e-03, 1.3360e-07,\n",
       "          3.7801e-05],\n",
       "         [4.4553e-02, 1.6476e-04, 3.6103e-05,  ..., 5.0072e-01, 1.7543e-03,\n",
       "          4.9246e-06],\n",
       "         [1.3393e-06, 1.4504e-01, 8.2162e-04,  ..., 7.9244e-07, 1.7903e-05,\n",
       "          3.0058e-03]], device='cuda:0'),\n",
       " 'fused_softmax_matmul': tensor([[8.7599e-08, 1.1568e-04, 8.5495e-05,  ..., 3.2460e-10, 3.2980e-08,\n",
       "          4.0416e-11],\n",
       "         [1.2988e-03, 8.0773e-06, 1.1501e-01,  ..., 7.0932e-04, 1.0986e-04,\n",
       "          9.1397e-04],\n",
       "         [6.8656e-04, 1.0944e-05, 1.6643e-04,  ..., 5.3404e-05, 6.4530e-06,\n",
       "          1.3815e-06],\n",
       "         ...,\n",
       "         [5.9146e-07, 3.2172e-05, 1.4822e-05,  ..., 2.2011e-03, 1.3360e-07,\n",
       "          3.7801e-05],\n",
       "         [4.4553e-02, 1.6476e-04, 3.6103e-05,  ..., 5.0072e-01, 1.7543e-03,\n",
       "          4.9246e-06],\n",
       "         [1.3393e-06, 1.4504e-01, 8.2162e-04,  ..., 7.9244e-07, 1.7903e-05,\n",
       "          3.0058e-03]], device='cuda:0')}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_allclose(kernels, \n",
    "sizes=(32, 16, 64),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1s matrices\n",
    "See that with matrices of one thoingd are fine for high tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.1444, 0.1444, 0.1444],\n",
      "        [0.1444, 0.1444, 0.1444],\n",
      "        [0.1444, 0.1444, 0.1444],\n",
      "        [0.1444, 0.1444, 0.1444],\n",
      "        [0.1444, 0.1444, 0.1444]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.1444, 0.1444, 0.1444],\n",
      "        [0.1444, 0.1444, 0.1444],\n",
      "        [0.1444, 0.1444, 0.1444],\n",
      "        [0.1444, 0.1444, 0.1444],\n",
      "        [0.1444, 0.1444, 0.1444]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333],\n",
      "        [0.3333, 0.3333, 0.3333]], device='cuda:0')\n",
      "{(3, 16, 65): True,\n",
      " (32, 2, 3): False,\n",
      " (32, 2, 31): True,\n",
      " (32, 2, 32): True,\n",
      " (32, 2, 56): True,\n",
      " (32, 2, 64): True,\n",
      " (32, 2, 65): True,\n",
      " (32, 16, 64): True,\n",
      " (32, 16, 65): True,\n",
      " (90, 2, 3): False,\n",
      " (90, 2, 31): True,\n",
      " (90, 2, 32): True,\n",
      " (90, 2, 56): True,\n",
      " (90, 2, 64): True,\n",
      " (90, 2, 65): True,\n",
      " (90, 32, 64): True,\n",
      " (90, 32, 65): True,\n",
      " (1024, 256, 56): True,\n",
      " (1024, 256, 768): True}\n"
     ]
    }
   ],
   "source": [
    "# Ones matrices\n",
    "results = {}\n",
    "for sizes in TEST_SIZES:\n",
    "    try:\n",
    "        N, L, M = sizes\n",
    "        test_allclose(kernels, raise_error=True, \n",
    "        size_show=5,\n",
    "        Q_small = torch.ones(N, L).contiguous().cuda(),\n",
    "        K_small = torch.ones(L, M).contiguous().cuda(),\n",
    "        verbose=False,\n",
    "        atol=1e-3,\n",
    "\n",
    "        )\n",
    "        results[sizes] = True\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        results[sizes] = False\n",
    "from pprint import  pprint\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test random matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul' failed:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "{(3, 16, 65): True,\n",
      " (32, 2, 3): True,\n",
      " (32, 2, 31): True,\n",
      " (32, 2, 32): True,\n",
      " (32, 2, 56): True,\n",
      " (32, 2, 64): True,\n",
      " (32, 2, 65): True,\n",
      " (32, 16, 64): True,\n",
      " (32, 16, 65): True,\n",
      " (90, 2, 3): True,\n",
      " (90, 2, 31): True,\n",
      " (90, 2, 32): True,\n",
      " (90, 2, 56): True,\n",
      " (90, 2, 64): True,\n",
      " (90, 2, 65): True,\n",
      " (90, 32, 64): True,\n",
      " (90, 32, 65): True,\n",
      " (1024, 256, 56): False,\n",
      " (1024, 256, 768): False}\n"
     ]
    }
   ],
   "source": [
    "#  Random matrices\n",
    "results = {}\n",
    "for sizes in TEST_SIZES:\n",
    "    try:\n",
    "        N, L, M = sizes\n",
    "        test_allclose(\n",
    "            kernels, \n",
    "            raise_error=True, \n",
    "            size_show=3,\n",
    "            sizes=sizes,\n",
    "            verbose=False,\n",
    "            use_int=True,\n",
    "            atol=1e-2,\n",
    "        )\n",
    "        results[sizes] = True\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        results[sizes] = False\n",
    "from pprint import  pprint\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[2.8097e-06, 1.7695e-06, 2.0064e-07],\n",
      "        [2.4593e-06, 8.6276e-06, 6.6717e-05],\n",
      "        [5.7646e-07, 1.5800e-06, 7.6471e-06]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0038, 0.0762, 0.0170],\n",
      "        [0.0011, 0.0651, 0.0190],\n",
      "        [0.0404, 0.0292, 0.0323]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0005, 0.0179, 0.2536],\n",
      "        [0.0324, 0.0333, 0.1303],\n",
      "        [0.0094, 0.0424, 0.1927]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0263, 0.0282, 0.0381],\n",
      "        [0.1206, 0.0048, 0.0009],\n",
      "        [0.0746, 0.0137, 0.0127]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0242, 0.0208, 0.0033],\n",
      "        [0.0012, 0.0020, 0.0171],\n",
      "        [0.0034, 0.0034, 0.0099]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[1.7743e-04, 2.3204e-01, 1.5667e-08],\n",
      "        [4.1152e-04, 3.5998e-04, 6.3054e-04],\n",
      "        [6.1604e-08, 2.1525e-05, 3.6943e-12]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[3.2226e-05, 3.0218e-05, 4.0610e-04],\n",
      "        [3.5384e-08, 5.2578e-06, 6.4309e-04],\n",
      "        [1.1268e-03, 2.0266e-05, 1.3631e-06]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0337, 0.0603, 0.0233],\n",
      "        [0.0355, 0.0187, 0.0351],\n",
      "        [0.0276, 0.0026, 0.0723]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0449, 0.0038, 0.0036],\n",
      "        [0.0198, 0.0447, 0.0169],\n",
      "        [0.0600, 0.0028, 0.0020]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0243, 0.0252, 0.0248],\n",
      "        [0.0281, 0.0197, 0.0153],\n",
      "        [0.0109, 0.0168, 0.0207]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[0.0277, 0.0140, 0.0128],\n",
      "        [0.0052, 0.0009, 0.0098],\n",
      "        [0.0004, 0.0961, 0.0003]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "kernel_name='fused_softmax_matmul'  rows do not sum up to 1:\n",
      "\n",
      " O[:size_show, :size_show]=tensor([[9.5715e-07, 2.3019e-06, 1.6489e-04],\n",
      "        [4.8285e-03, 5.5008e-06, 6.7034e-07],\n",
      "        [6.1058e-05, 7.2980e-07, 6.6638e-05]], device='cuda:0')\n",
      "\n",
      " O_torch[:size_show, :size_show]=tensor([[0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179],\n",
      "        [0.0179, 0.0179, 0.0179]], device='cuda:0')\n",
      "{'fused_softmax_matmul': tensor([[3.5067e-08, 7.3442e-14, 3.9828e-13,  ..., 7.0628e-03, 3.5043e-07,\n",
      "         3.1772e-09],\n",
      "        [2.3474e-05, 2.7816e-10, 2.8124e-11,  ..., 5.7908e-07, 5.6982e-05,\n",
      "         1.1500e-06],\n",
      "        [5.5624e-07, 4.1781e-07, 3.2425e-10,  ..., 1.5819e-06, 9.2846e-06,\n",
      "         4.8701e-09],\n",
      "        ...,\n",
      "        [1.6960e-06, 2.1360e-06, 3.1504e-07,  ..., 5.8527e-08, 9.8355e-06,\n",
      "         2.4244e-07],\n",
      "        [3.3277e-10, 2.8571e-09, 7.8954e-13,  ..., 1.7839e-06, 2.2782e-06,\n",
      "         7.0610e-10],\n",
      "        [3.7331e-06, 1.7603e-07, 1.1849e-04,  ..., 1.3174e-03, 2.0982e-05,\n",
      "         1.0139e-09]], device='cuda:0'),\n",
      " (3, 16, 65): False,\n",
      " (32, 2, 3): False,\n",
      " (32, 2, 31): False,\n",
      " (32, 2, 32): True,\n",
      " (32, 2, 56): False,\n",
      " (32, 2, 64): True,\n",
      " (32, 2, 65): False,\n",
      " (32, 16, 64): True,\n",
      " (32, 16, 65): False,\n",
      " (90, 2, 3): False,\n",
      " (90, 2, 31): False,\n",
      " (90, 2, 32): True,\n",
      " (90, 2, 56): False,\n",
      " (90, 2, 64): True,\n",
      " (90, 2, 65): False,\n",
      " (90, 32, 64): True,\n",
      " (90, 32, 65): False,\n",
      " (1024, 256, 56): False,\n",
      " (1024, 256, 768): True}\n"
     ]
    }
   ],
   "source": [
    "#  Random matrices\n",
    "results = {}\n",
    "for sizes in TEST_SIZES:\n",
    "    try:\n",
    "        N, L, M = sizes\n",
    "        test_is_normalised(\n",
    "            kernels, \n",
    "            raise_error=True, \n",
    "            size_show=3,\n",
    "            sizes=sizes,\n",
    "            verbose=False,\n",
    "            use_int=False,\n",
    "        )\n",
    "        results[sizes] = True\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        results[sizes] = False\n",
    "from pprint import  pprint\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
