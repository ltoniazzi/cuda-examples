{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fused softmax + matmul\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory of the current notebook to sys.path\n",
    "cur_dir = Path().resolve()\n",
    "parent_dir = cur_dir.parent\n",
    "sys.path += [str(parent_dir), str(cur_dir)]\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import cdiv, get_sig, load_cuda, profile_kernel\n",
    "from collections import namedtuple\n",
    "\n",
    "def test_allclose(kernels):\n",
    "    D = 1024\n",
    "    V_test = torch.randn(D).contiguous().cuda()\n",
    "    O_torch = torch.softmax(V_test,  dim=0)\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        if kernel_name!=\"torch\":\n",
    "            module, fname = kernel_data[\"module\"], kernel_data[\"fname\"]\n",
    "            O = getattr(module, fname)(V_test)\n",
    "            if not torch.allclose(O, O_torch, atol=1e-4):\n",
    "                raise ValueError(f\"{kernel_name=} failed:\\n\\n {O[:10]=}, {O_torch[:10]=}\")\n",
    "            print(f\"{kernel_name=} agrees with torch softmax\")\n",
    "        \n",
    "\n",
    "\n",
    "def profile_kernels(kernels):\n",
    "    test_allclose(kernels)\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        print(f\"Profiling: {kernel_name}\")\n",
    "        profile_kernel(kernel_data[\"module\"], kernel_data[\"fname\"], *kernel_data[\"args\"], **kernel_data[\"kwargs\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python cuda looking implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Q shape: torch.Size([32, 16])\n",
      "Input K shape: torch.Size([16, 64])\n",
      "Output shape : torch.Size([32, 64])\n",
      "Output: tensor([[3.2414e-02, 2.9322e-05, 2.3462e-05,  ..., 2.6289e-06, 2.4236e-03,\n",
      "         3.9168e-07],\n",
      "        [1.9082e-03, 3.4305e-05, 6.3105e-05,  ..., 1.5615e-03, 3.3092e-03,\n",
      "         1.2458e-06],\n",
      "        [1.3428e-02, 2.9178e-04, 8.2823e-06,  ..., 3.0067e-03, 1.0007e-03,\n",
      "         1.9285e-02],\n",
      "        ...,\n",
      "        [4.8037e-01, 2.4345e-04, 2.5271e-10,  ..., 5.0420e-05, 5.3735e-04,\n",
      "         3.4535e-07],\n",
      "        [1.3780e-02, 7.5145e-05, 1.8623e-03,  ..., 2.1507e-03, 3.6669e-05,\n",
      "         8.2915e-02],\n",
      "        [1.5153e-05, 5.6218e-04, 5.1192e-05,  ..., 1.2755e-02, 7.1740e-04,\n",
      "         3.6581e-04]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "N, L, M = 32, 16, 64\n",
    "Q_small = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "K_small = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "\n",
    "class TorchNaiveSoftmaxAndMatMul(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def softmax_and_matmul(self, Q, K):\n",
    "        O = Q@K\n",
    "        return torch.softmax(O, dim=0)\n",
    "\n",
    "model = TorchNaiveSoftmaxAndMatMul()\n",
    "output = model.softmax_and_matmul(Q_small, K_small)\n",
    "\n",
    "print(\"Input Q shape:\", Q_small.shape)\n",
    "print(\"Input K shape:\", K_small.shape)\n",
    "print(\"Output shape :\", output.shape)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modules(kernels):\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "\n",
    "        fname = kernel_data[\"fname\"]\n",
    "        cuda_source = Path(kernel_data[\"cuda_source_path\"]).read_text()\n",
    "        cpp_source = get_sig(fname, cuda_source)\n",
    "        module = load_cuda(cuda_source, cpp_source, funcs=[fname])\n",
    "        kernel_data[\"module\"] = module\n",
    "\n",
    "\n",
    "def get_softmax_modules(kernels):\n",
    "    get_modules(kernels)\n",
    "    kernels[\"torch\"] = {\n",
    "        \"module\": TorchNaiveSoftmaxAndMatMul(),\n",
    "        \"fname\": \"softmax_and_matmul\",\n",
    "    }\n",
    "\n",
    "def add_args_kwargs(kernels, *args, **kwargs):\n",
    "    for kernel_name, kernel_data in kernels.items():\n",
    "        kernel_data[\"args\"]= args\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/Workspace/cuda-examples/.venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "N, L, M = 1024, 256, 768\n",
    "Q = torch.randn(N, L, dtype=torch.float32).contiguous().cuda()\n",
    "K = torch.randn(L, M, dtype=torch.float32).contiguous().cuda()\n",
    "\n",
    "kernels = {\n",
    "    \"fused_softmax_matmul\": dict(cuda_source_path = \"./fused_softmax_matmul.cu\", fname = \"fused_softmax_matmul\"),\n",
    "}\n",
    "get_softmax_modules(kernels)\n",
    "add_args_kwargs(kernels, Q, K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fused_softmax_matmul(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: torch.Tensor, arg1: torch.Tensor) -> torch.Tensor\n\nInvoked with: tensor([ 0.4030, -0.5492,  0.0681,  ...,  1.1203, -0.3838,  2.1356],\n       device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprofile_kernels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernels\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mprofile_kernels\u001b[39m\u001b[34m(kernels)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprofile_kernels\u001b[39m(kernels):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mtest_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m kernel_name, kernel_data \u001b[38;5;129;01min\u001b[39;00m kernels.items():\n\u001b[32m     22\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProfiling: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtest_allclose\u001b[39m\u001b[34m(kernels)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kernel_name!=\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     11\u001b[39m     module, fname = kernel_data[\u001b[33m\"\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m\"\u001b[39m], kernel_data[\u001b[33m\"\u001b[39m\u001b[33mfname\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     O = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.allclose(O, O_torch, atol=\u001b[32m1e-4\u001b[39m):\n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_name\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m failed:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mO[:\u001b[32m10\u001b[39m]\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mO_torch[:\u001b[32m10\u001b[39m]\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: fused_softmax_matmul(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: torch.Tensor, arg1: torch.Tensor) -> torch.Tensor\n\nInvoked with: tensor([ 0.4030, -0.5492,  0.0681,  ...,  1.1203, -0.3838,  2.1356],\n       device='cuda:0')"
     ]
    }
   ],
   "source": [
    "profile_kernels(kernels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
