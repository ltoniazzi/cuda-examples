{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention in Torch, Numba and Cuda\n",
    "\n",
    "\n",
    "We implement in 3 different ways the forward algorithm from the [Flash Attention 2 paper](https://arxiv.org/pdf/2307.08691)\n",
    "\n",
    "We build the kernel for `d=128` and design it so that it computes the full attention in a single block.\n",
    "\n",
    "![./flash_attention_fwd.png](./flash_attention_fwd.png)\n",
    "\n",
    "todo: \n",
    "- registers calculation, show spills\n",
    "- investigate why larger matrices slower with profiler? allegedly it becoems comput intensive as we use only 1 block, prove that?\n",
    "- mention no thunder example\n",
    "- fix logsumexp calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba.cuda import as_cuda_array as ca\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import sys, os\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "sys.path.insert(0, '../..')\n",
    "from utils import load_cuda, get_sig\n",
    "\n",
    "import os\n",
    "\n",
    "def get_loaded_cuda_module(fname, verbose=False):\n",
    "    cuda_src_path = f\"./{fname}.cu\"\n",
    "    cuda_src = Path(cuda_src_path).read_text()\n",
    "    cpp_src = get_sig(fname, cuda_src)\n",
    "    return load_cuda(cuda_src, cpp_src, [fname], verbose=verbose)\n",
    "\n",
    "def profile_kernel(module, fname, *args, **kwargs):\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                with_stack=True, record_shapes=True) as prof:\n",
    "        torch.cuda.synchronize()\n",
    "        getattr(module, fname)(*args, **kwargs)\n",
    "        torch.cuda.synchronize()\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(7.4506e-08, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test tensors\n",
    "def get_test_tensors(N_inp, N_out, d):\n",
    "    Q = torch.randn(N_out, d).contiguous().to(\"cuda\")\n",
    "    K = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    V = torch.randn(N_inp, d).contiguous().to(\"cuda\")\n",
    "    scaling = 1.0 / math.sqrt(d)\n",
    "    return Q, K, V, scaling\n",
    "\n",
    "N_inp = 16\n",
    "N_out = 16\n",
    "d = 2\n",
    "\n",
    "Q, K, V, scaling = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "# Get expected O\n",
    "O_expected = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "# Get expected L\n",
    "S = (Q @ K.T) * scaling  # shape: (N_out, N_in)\n",
    "max_per_row, _ = torch.max(S, dim=1, keepdim=True)  # shape: (N_out, 1)\n",
    "exp_shifted = torch.exp(S - max_per_row)  # shape: (N_out, N_inp)\n",
    "L_expected = torch.sum(exp_shifted, dim=-1)\n",
    "L_expected2 = max_per_row + torch.log(L_expected)\n",
    "L_expected3 = max_per_row + torch.logsumexp(S - max_per_row, dim=1)\n",
    "\n",
    "assert (L_expected2-L_expected3).abs().max() < 5*1e-10\n",
    "\n",
    "def check_diff(O, L=None, atol=5*1e-5):\n",
    "    O_diff = (O-O_expected).abs().max()\n",
    "    print(\"Max absolute difference:\")\n",
    "    if atol:\n",
    "        assert O_diff < atol, f\"O diff too large: {O_diff} > {atol=}\"\n",
    "    print(\"O: \", O_diff)\n",
    "    if L is not None:\n",
    "        L_diff = (L.squeeze()-L_expected).abs().max()\n",
    "        if atol:\n",
    "            assert L_diff < atol, f\"L diff too large: {L_diff} > {atol=}\"\n",
    "        print(\"L: \", L_diff)\n",
    "\n",
    "\n",
    "check_diff(O=torch.softmax(Q @ K.T * scaling, dim=-1) @ V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_torch(Q, K, V, O, L, N_inp, N_out, d) -> None:\n",
    "    \"\"\"Forward algo from https://arxiv.org/pdf/2307.08691\n",
    "    \"\"\"\n",
    "\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    scaling = 1 / math.sqrt(d)\n",
    "\n",
    "    # Q and O L split into T_r; K, V in T_c blocks\n",
    "    for i in range(T_r):\n",
    "        Q_i = Q[i * B_r : (i + 1) * B_r]\n",
    "        O_i = torch.zeros(B_r, d)\n",
    "        L_i = torch.zeros(B_r, 1)\n",
    "        m_i = torch.full((B_r, 1), -math.inf)\n",
    "        last_m_i = m_i\n",
    "        for j in range(T_c):\n",
    "            K_j = K[j * B_c : (j + 1) * B_c]\n",
    "            V_j = V[j * B_c : (j + 1) * B_c]\n",
    "            S_i = scaling * (Q_i @ K_j.T)\n",
    "            m_i = torch.maximum(m_i, S_i.max(dim=-1, keepdim=True).values)\n",
    "            P_i = torch.exp(S_i - m_i)\n",
    "            L_i = torch.exp(last_m_i - m_i) * L_i + P_i.sum(dim=-1, keepdim=True)\n",
    "            O_i = torch.exp(last_m_i - m_i) * O_i + P_i @ V_j\n",
    "            last_m_i = m_i\n",
    "        O_i = (1.0 / L_i) * O_i\n",
    "        O[i * B_r : (i + 1) * B_r] = O_i\n",
    "        L[i * B_r : (i + 1) * B_r] = L_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(2.3842e-07, device='cuda:0')\n",
      "L:  tensor(1.4305e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "O_torch_loop = torch.zeros(N_out, d)\n",
    "L_torch_loop = torch.zeros(N_out, 1)\n",
    "\n",
    "flash_attention_torch(Q.to(\"cpu\"), K.to(\"cpu\"), V.to(\"cpu\"), O_torch_loop, L_torch_loop, N_inp, N_out, d)\n",
    "\n",
    "check_diff(\n",
    "    O_torch_loop.to(\"cuda\"), \n",
    "    L_torch_loop.to(\"cuda\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.0635],\n",
       "         [ 7.4613],\n",
       "         [ 6.6346],\n",
       "         [ 5.6546],\n",
       "         [ 9.0227],\n",
       "         [ 2.6117],\n",
       "         [14.6637],\n",
       "         [ 6.4889],\n",
       "         [ 6.8281],\n",
       "         [ 5.6799],\n",
       "         [ 7.3275],\n",
       "         [ 7.7920],\n",
       "         [ 1.8350],\n",
       "         [ 6.0733],\n",
       "         [ 5.2760],\n",
       "         [ 4.3893],\n",
       "         [ 2.3933],\n",
       "         [ 8.6296],\n",
       "         [ 4.9935],\n",
       "         [ 5.8907],\n",
       "         [ 5.5498],\n",
       "         [ 7.6121],\n",
       "         [ 3.0597],\n",
       "         [ 3.9153],\n",
       "         [ 2.2110],\n",
       "         [ 7.2019],\n",
       "         [ 8.8995],\n",
       "         [ 6.7409],\n",
       "         [ 3.8753],\n",
       "         [ 6.3121],\n",
       "         [ 9.9120],\n",
       "         [ 5.0796]]),\n",
       " tensor([[4.0131, 4.0679, 3.9505,  ..., 3.9006, 4.3519, 3.6834],\n",
       "         [3.8243, 3.8791, 3.7617,  ..., 3.7118, 4.1631, 3.4946],\n",
       "         [3.7784, 3.8332, 3.7157,  ..., 3.6659, 4.1172, 3.4487],\n",
       "         ...,\n",
       "         [4.4671, 4.5219, 4.4045,  ..., 4.3547, 4.8060, 4.1374],\n",
       "         [3.5048, 3.5596, 3.4422,  ..., 3.3924, 3.8436, 3.1751],\n",
       "         [4.1407, 4.1955, 4.0781,  ..., 4.0282, 4.4795, 3.8110]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_torch_loop, L_expected2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.cuda.jit\n",
    "def attention_numba_spilling(Q, K, V, scaling: numba.float32, L, O):\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "    \n",
    "    # These can be in registers but wont fit too large\n",
    "    # Also here they spill from SMEM\n",
    "    l_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    m_i = numba.cuda.shared.array((B_r,), inp_dtype)\n",
    "    O_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, numba.cuda.blockDim.x):\n",
    "                for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii][jj])\n",
    "                m_i[ii] = m\n",
    "                l = math.exp(last_m - m) * l_i[ii]\n",
    "\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    O_i[ii, dd] *= math.exp(last_m - m)\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = math.exp(S[ii][jj] - m)  # Cache...\n",
    "                    l += P_ij\n",
    "                    for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd]\n",
    "                l_i[ii] = l\n",
    "                \n",
    "        numba.cuda.syncthreads()\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):  \n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                O[ii + i * B_r, dd] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii + i * B_r] = l_i[ii]   \n",
    "        numba.cuda.syncthreads() \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(5.9605e-07, device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "L diff too large: 2.4368085861206055 > atol=5e-05",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m grid \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,)\n\u001b[1;32m      5\u001b[0m attention_numba_spilling[grid, tpb](Q, K, V, scaling, L_splilling, O_splilling)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mcheck_diff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mO_splilling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL_splilling\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m, in \u001b[0;36mcheck_diff\u001b[0;34m(O, L, atol)\u001b[0m\n\u001b[1;32m     34\u001b[0m L_diff \u001b[38;5;241m=\u001b[39m (L\u001b[38;5;241m-\u001b[39mL_expected3)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m atol:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m L_diff \u001b[38;5;241m<\u001b[39m atol, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL diff too large: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL_diff\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m > \u001b[39m\u001b[38;5;132;01m{\u001b[39;00matol\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL: \u001b[39m\u001b[38;5;124m\"\u001b[39m, L_diff)\n",
      "\u001b[0;31mAssertionError\u001b[0m: L diff too large: 2.4368085861206055 > atol=5e-05"
     ]
    }
   ],
   "source": [
    "O_splilling = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "L_splilling = torch.zeros(N_out, device=\"cuda\")\n",
    "tpb = (8, 16)\n",
    "grid = (1,)\n",
    "attention_numba_spilling[grid, tpb](Q, K, V, scaling, L_splilling, O_splilling)\n",
    "check_diff(O_splilling, L_splilling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7684e-07, -4.7684e-07,  2.3842e-07,  ...,  2.3842e-07,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [-4.7684e-07, -2.3842e-07,  4.7684e-07,  ...,  2.3842e-07,\n",
       "          4.7684e-07, -2.3842e-07],\n",
       "        [-2.3842e-07, -4.7684e-07,  2.3842e-07,  ...,  4.7684e-07,\n",
       "          4.7684e-07, -2.3842e-07],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  0.0000e+00,  4.7684e-07,  ...,  4.7684e-07,\n",
       "          4.7684e-07,  0.0000e+00],\n",
       "        [-2.3842e-07, -2.3842e-07,  2.3842e-07,  ...,  2.3842e-07,\n",
       "          2.3842e-07,  0.0000e+00],\n",
       "        [-4.7684e-07,  0.0000e+00,  4.7684e-07,  ...,  4.7684e-07,\n",
       "          4.7684e-07, -2.3842e-07]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_expected3 - (max_per_row+torch.log(L_splilling))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make local threads arrays small as to make it more likely they will be in registers.\n",
    "\n",
    "We have 1 block of size `(block_dim_x, block_dim_x)` Then for each ...\n",
    "\n",
    "So we have \n",
    "```\n",
    "B_r / block_dim_y * 2 + B_r * d / block_dim_x = 2 * 16 / 4 + 16 * 128 / 32 = 72\n",
    "```\n",
    "so 72 single pecision per thread -> (72 * 32 * 4) * 4bytes = 9216*4bytes= 36864 bytes = 36KB << 64KB register memory per SM.\n",
    "\n",
    "\n",
    "If we do not shrink those we get\n",
    "```\n",
    "B_r * 2 + B_r *. d = 3 * 16 + 16* 128 = 2096 -> (2096 * 32 * 4) * 4 = 1MB > 64KB -> spill\n",
    "```\n",
    "\n",
    "Shared memory has enough to host 2096 * 4 = 8384 = 8KB extra? \n",
    "as already\n",
    "Br*d + 2 Bc * d + bc*Br = 6400 -> 25KB < 48KB\n",
    "\n",
    "But it;s slower than registers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25600"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 16 / 4 + 16 * 128 / 32\n",
    "72 * 32 * 4 * 4\n",
    "36864/1024\n",
    "3 * 16 + 16* 128\n",
    "2096 * 32 * 4 * 4 /1024\n",
    "2096 * 4\n",
    "(16*128*2 + 16*128 + 16*16)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No spilling: make local arrays small\n",
    "block_dim_x = 32\n",
    "block_dim_y = 4\n",
    "B_r = 16\n",
    "o_per_thread_x = d // block_dim_x\n",
    "o_per_thread_y = B_r // block_dim_y\n",
    "\n",
    "@numba.cuda.jit\n",
    "def attention_numba_no_spilling(Q, K, V, scaling: numba.float32, L, O):\n",
    "    B_c = 16\n",
    "    \n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "    \n",
    "    # These can be in registers\n",
    "    l_i = numba.cuda.local.array((o_per_thread_y,), inp_dtype)\n",
    "    m_i = numba.cuda.local.array((o_per_thread_y,), inp_dtype)\n",
    "    O_i = numba.cuda.local.array((o_per_thread_y, o_per_thread_x), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "                O_i[ii//block_dim_y, dd//block_dim_x] = 0\n",
    "            l_i[ii//block_dim_y] = 0\n",
    "            m_i[ii//block_dim_y] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scaling * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, numba.cuda.blockDim.x):\n",
    "                for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scaling * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "                # torch.maximum(m_i, S_i.max(dim=-1, keepdim=True).values)\n",
    "                # this needs to use the parallel reduction pattern\n",
    "                m = m_i[ii//block_dim_y]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii][jj])\n",
    "                m_i[ii//block_dim_y] = m\n",
    "                l = math.exp(last_m - m) * l_i[ii//block_dim_y]\n",
    "\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    O_i[ii//block_dim_y, dd//block_dim_x] *= math.exp(last_m - m)\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = math.exp(S[ii][jj] - m)  # Cache...\n",
    "                    l += P_ij\n",
    "                    for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                        O_i[ii//block_dim_y, dd//block_dim_x] += P_ij * V_j[jj, dd]\n",
    "                l_i[ii//block_dim_y] = l\n",
    "        numba.cuda.syncthreads()\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):  \n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                O[ii + i * B_r, dd] = O_i[ii//block_dim_y, dd//block_dim_x] / l_i[ii//block_dim_y]\n",
    "            L[ii + i * B_r] = l_i[ii//block_dim_y]   \n",
    "        numba.cuda.syncthreads() \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(4.1723e-07, device='cuda:0')\n",
      "L:  tensor(3.8147e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "O_no_spilling = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "L_no_spilling = torch.zeros(N_out, device=\"cuda\")\n",
    "tpb = (block_dim_x, block_dim_y)\n",
    "grid = (1,)\n",
    "attention_numba_no_spilling[grid, tpb](Q, K, V, scaling, L_no_spilling, O_no_spilling)\n",
    "check_diff(O_no_spilling, L_no_spilling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cuda\n",
    "\n",
    "### Register spilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2073: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fname = \"flash_attention\"\n",
    "module_cuda = get_loaded_cuda_module(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                      flash_attention_k         0.00%       0.000us         0.00%       0.000us       0.000us      48.000us        94.76%      48.000us      48.000us             1  \n",
      "                                            aten::zeros         0.81%      16.531us         6.33%     129.798us      64.899us       0.000us         0.00%       2.656us       1.328us             2  \n",
      "                                            aten::zero_         0.36%       7.410us         4.13%      84.735us      42.367us       0.000us         0.00%       2.656us       1.328us             2  \n",
      "                                            aten::fill_         0.99%      20.281us         3.77%      77.325us      38.663us       2.656us         5.24%       2.656us       1.328us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.656us         5.24%       2.656us       1.328us             2  \n",
      "                                           Unrecognized        88.73%       1.819ms        88.73%       1.819ms     454.863us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                  cudaDeviceSynchronize         1.37%      28.081us         1.37%      28.081us       9.360us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                            aten::empty         1.39%      28.532us         1.39%      28.532us      14.266us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                       cudaLaunchKernel         6.35%     130.279us        15.43%     316.310us     105.437us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.051ms\n",
      "Self CUDA time total: 50.656us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_kernel(module_cuda, fname, Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(0.0997, device='cuda:0')\n",
      "L:  tensor(6.6757e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "O_cuda, L_cuda = getattr(module_cuda, fname)(Q, K, V)\n",
    "check_diff(O_cuda, L_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Registers spilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2073: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fname_spill_from_registers = \"flash_attention_spilling_from_registers\"\n",
    "module_cuda_spilling_from_registers = get_loaded_cuda_module(fname_spill_from_registers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2073: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fname_spill_from_smem = \"flash_attention_spilling_from_smem\"\n",
    "module_cuda_spilling_from_smem = get_loaded_cuda_module(fname_spill_from_smem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                      flash_attention_k         0.00%       0.000us         0.00%       0.000us       0.000us      46.560us        95.16%      46.560us      46.560us             1  \n",
      "                                            aten::zeros         0.87%      18.211us         7.24%     151.370us      75.685us       0.000us         0.00%       2.367us       1.184us             2  \n",
      "                                            aten::zero_         0.47%       9.821us         4.89%     102.287us      51.143us       0.000us         0.00%       2.367us       1.184us             2  \n",
      "                                            aten::fill_         1.19%      24.961us         4.42%      92.466us      46.233us       2.367us         4.84%       2.367us       1.184us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.367us         4.84%       2.367us       1.184us             2  \n",
      "                                           Unrecognized        87.97%       1.840ms        87.97%       1.840ms     460.016us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                  cudaDeviceSynchronize         1.66%      34.643us         1.66%      34.643us      11.548us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "                                            aten::empty         1.48%      30.872us         1.48%      30.872us      15.436us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                       cudaLaunchKernel         6.37%     133.228us        15.03%     314.430us     104.810us       0.000us         0.00%       0.000us       0.000us             3  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.092ms\n",
      "Self CUDA time total: 48.927us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile_kernel(module_cuda_spilling_from_smem, fname_spill_from_smem, Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference:\n",
      "O:  tensor(5.9605e-07, device='cuda:0')\n",
      "L:  tensor(6.6757e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "O_cuda_spilling, L_cuda_spilling = getattr(module_cuda_spilling_from_smem, fname_spill_from_smem)(Q, K, V)\n",
    "check_diff(O_cuda_spilling, L_cuda_spilling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test performance on different dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=32, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 μs ± 2.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Flash Attention\n",
      "111 μs ± 661 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Flash Attention: spill from SMEM\n",
      "109 μs ± 865 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Flash Attention: spill from registers\n",
      "350 μs ± 1.17 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=64, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "210 μs ± 3.23 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Flash Attention\n",
      "155 μs ± 430 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Flash Attention: spill from SMEM\n",
      "151 μs ± 609 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "\n",
      "- Flash Attention: spill from registers\n",
      "628 μs ± 1.22 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=128, N_inp=128, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "209 μs ± 1.98 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Flash Attention\n",
      "784 μs ± 534 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Flash Attention: spill from SMEM\n",
      "758 μs ± 1.62 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Flash Attention: spill from registers\n",
      "4.53 ms ± 1.9 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=512, N_inp=512, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "241 μs ± 3.09 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "- Flash Attention\n",
      "11.5 ms ± 1.97 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "- Flash Attention: spill from SMEM\n",
      "11 ms ± 1.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "- Flash Attention: spill from registers\n",
      "70.9 ms ± 12.5 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "TEST_DIMS = [\n",
    "    (32, 32),\n",
    "    (64, 32),\n",
    "    (128, 128),\n",
    "    (512, 512),\n",
    "]\n",
    "for N_inp, N_out in TEST_DIMS:\n",
    "    Q, K, V, _ = get_test_tensors(N_inp, N_out, d)\n",
    "    print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"\\n- Torch scaled_dot_product_attention\")\n",
    "    %timeit torch.nn.functional.scaled_dot_product_attention(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Flash Attention\")\n",
    "    %timeit getattr(module_cuda, fname)(Q, K, V); torch.cuda.synchronize()\n",
    "    # print(\"\\n- Flash Attention: spill from SMEM\")\n",
    "    # %timeit getattr(module_cuda_spilling_from_smem, fname_spill_from_smem)(Q, K, V); torch.cuda.synchronize()\n",
    "    print(\"\\n- Flash Attention: spill from registers\")\n",
    "    %timeit getattr(module_cuda_spilling_from_registers, fname_spill_from_registers)(Q, K, V); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "\u0000\n",
      "0\n",
      "0 <CUfunction 0x55ce7273cdc0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([94345151665600, 94345151665608, 94345151665616, 94345151665624,\n",
       "        94345151665632, 94345172775232, 94345116918272, 94345116918276,\n",
       "        94345116918280], dtype=torch.uint64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create program\n",
    "# conda install cuda-python\n",
    "from cuda import cuda, nvrtc\n",
    "\n",
    "cuda_src_path = f\"./{fname}.cu\"\n",
    "import re\n",
    "cuda_src = Path(cuda_src_path).read_text()\n",
    "cuda_src = re.sub(r'__host__.*', '', cuda_src, flags=re.DOTALL)\n",
    "cuda_src = cuda_src.replace(\"#include <math_constants.h>\",  \"\")\n",
    "\n",
    "N_inp = 32\n",
    "N_out = 32\n",
    "d = 128\n",
    "B_r, B_c = 16, 16\n",
    "T_r = (N_out + B_r -1) // B_r\n",
    "T_c = (N_inp + B_r -1) // B_c\n",
    "Q, K, V, scale_factor = get_test_tensors(N_inp, N_out, d)\n",
    "\n",
    "err, prog = nvrtc.nvrtcCreateProgram(str.encode(cuda_src), b\"flash_attention.cu\", 0, [], [])\n",
    "\n",
    "# Compile program\n",
    "min, maj = torch.cuda.get_device_capability()\n",
    "opts = [f\"--gpu-architecture=compute_{min}{maj}\".encode()]\n",
    "err, = nvrtc.nvrtcCompileProgram(prog, len(opts), opts)\n",
    "\n",
    "print(err)\n",
    "\n",
    "# Get PTX from compilation\n",
    "err, ptxSize = nvrtc.nvrtcGetPTXSize(prog)\n",
    "ptx = b\" \" * ptxSize\n",
    "err, = nvrtc.nvrtcGetPTX(prog, ptx)\n",
    "print(err)\n",
    "\n",
    "err, logSize = nvrtc.nvrtcGetProgramLogSize(prog)\n",
    "log = b\" \" * logSize\n",
    "err, = nvrtc.nvrtcGetProgramLog(prog, log)\n",
    "print(log.decode())\n",
    "# print(ptx.decode())\n",
    "\n",
    "# Load PTX as module data and retrieve function\n",
    "err, module = cuda.cuModuleLoadData(ptx)\n",
    "print(err)\n",
    "err, kernel = cuda.cuModuleGetFunction(module, b\"flash_attention_k\")\n",
    "print(err, kernel)\n",
    "\n",
    "# Allocate tensors\n",
    "# S3 = torch.zeros(N_out, N_out, device=\"cuda\")\n",
    "O_cuda_py = torch.zeros(N_out, d, device=\"cuda\")\n",
    "L_cuda_py = torch.zeros(N_out, device=\"cuda\")\n",
    "\n",
    "# To quote the official tutorial: (https://nvidia.github.io/cuda-python/overview.html)\n",
    "# The following code example is not intuitive\n",
    "# Subject to change in a future release\n",
    "\n",
    "int_args = torch.tensor([0, T_r, T_c], dtype=torch.int32)\n",
    "float_args = torch.tensor([scale_factor], dtype=torch.float32)\n",
    "ptr_args = torch.tensor([i.data_ptr() for i in (O_cuda_py, L_cuda_py, K, Q, V)], dtype=torch.uint64)\n",
    "\n",
    "args = torch.tensor([\n",
    "    *(i.data_ptr() for i in ptr_args),\n",
    "    *(i.data_ptr() for i in float_args),\n",
    "    *(i.data_ptr() for i in int_args)], dtype=torch.uint64)\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.3644e-07, device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_expected = torch.nn.functional.scaled_dot_product_attention(Q, K, V)\n",
    "def fn():\n",
    "    err = cuda.cuLaunchKernel(\n",
    "        kernel,\n",
    "        1,  # grid x dim\n",
    "        1,  # grid y dim\n",
    "        1,  # grid z dim\n",
    "        32,  # block x dim\n",
    "        32,  # block y dim\n",
    "        1,  # block z dim\n",
    "        0,  # dynamic shared memory\n",
    "        torch.cuda.current_stream().stream_id,  # stream\n",
    "        args.data_ptr(),  # kernel arguments\n",
    "        0,  # extra (ignore)\n",
    "    )\n",
    "\n",
    "fn()\n",
    "\n",
    "(O_cuda_py - O_expected).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**********\n",
      "Dimensions: N_out=32, N_inp=32, d=128\n",
      "\n",
      "- Torch scaled_dot_product_attention\n",
      "69.6 μs ± 383 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n\\n**********\\nDimensions: {N_out=}, {N_inp=}, {d=}\")\n",
    "torch.cuda.synchronize()\n",
    "print(\"\\n- Torch scaled_dot_product_attention\")\n",
    "%timeit fn(); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åppendix\n",
    "TODO: for large matrices numerical error is too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda setup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
