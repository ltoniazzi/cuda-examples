{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash attention in python, numba and cuda\n",
    "\n",
    "todo: registers calculation, show spills and run both versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba.cuda import as_cuda_array as ca\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import sys, os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "sys.path.insert(0, '../..')\n",
    "from utils import load_cuda, cuda_begin, cdiv, get_sig\n",
    "\n",
    "def cdiv(a,b):\n",
    "    \"Int ceiling division of `a` over `b`\"\n",
    "    return (a+b-1)//b\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tensors\n",
    "N_inp = 32\n",
    "N_out = 32\n",
    "d = 128\n",
    "Q = torch.randn(N_out, d).contiguous()\n",
    "K = torch.randn(N_inp, d).contiguous()\n",
    "V = torch.randn(N_inp, d).contiguous()\n",
    "Kc = K.to(\"cuda\")\n",
    "Qc = Q.to(\"cuda\")\n",
    "Vc = V.to(\"cuda\")\n",
    "scaling = 1.0 / math.sqrt(d)\n",
    "\n",
    "# Get expected O\n",
    "O_expected = torch.softmax(Q @ K.T * scaling, dim=-1) @ V\n",
    "# Get expected L\n",
    "S = (Q @ K.T) * scaling  # shape: (N_out, N_in)\n",
    "max_per_row, _ = torch.max(S, dim=1, keepdim=True)  # shape: (N_out, 1)\n",
    "exp_shifted = torch.exp(S - max_per_row)  # shape: (N_out, N_in)\n",
    "L_expected = torch.sum(exp_shifted, dim=1)\n",
    "\n",
    "def check_diff(O, L):\n",
    "    print(\"Max absolute difference O: \", (O-O_expected).abs().max())\n",
    "    print(\"Max absolute difference L: \", (L-L_expected).abs().max())\n",
    "\n",
    "\n",
    "# O_expected2 = torch.nn.att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_torch(Q, K, V, O, L, N_inp, N_out, d) -> None:\n",
    "    \"\"\"Forward algo from https://arxiv.org/pdf/2307.08691\n",
    "    \"\"\"\n",
    "\n",
    "    B_c = min(16, N_inp)\n",
    "    B_r = min(16, N_out)\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "\n",
    "    scale_factor = 1 / math.sqrt(Q.size(-1))\n",
    "\n",
    "    # Q and O L split into T_r; K, V in T_c blocks\n",
    "    for i in range(T_r):\n",
    "        Q_i = Q[i * B_r : (i + 1) * B_r]\n",
    "        O_i = torch.zeros(B_r, d)\n",
    "        L_i = torch.zeros(B_r, 1)\n",
    "        m_i = torch.full((B_r, 1), -math.inf)\n",
    "        last_m_i = m_i\n",
    "        for j in range(T_c):\n",
    "            K_j = K[j * B_c : (j + 1) * B_c]\n",
    "            V_j = V[j * B_c : (j + 1) * B_c]\n",
    "            S_i = scale_factor * (Q_i @ K_j.T)\n",
    "            m_i = torch.maximum(m_i, S_i.max(dim=-1, keepdim=True).values)\n",
    "            P_i = torch.exp(S_i - m_i)\n",
    "            L_i = torch.exp(last_m_i - m_i) * L_i + P_i.sum(dim=-1, keepdim=True)\n",
    "            O_i = torch.exp(last_m_i - m_i) * O_i + P_i @ V_j\n",
    "            last_m_i = m_i\n",
    "        O_i = (1.0 / L_i) * O_i\n",
    "        L_i = m_i + torch.log(L_i)\n",
    "        O[i * B_r : (i + 1) * B_r] = O_i\n",
    "        L[i * B_r : (i + 1) * B_r] = L_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference O:  tensor(2.3842e-07)\n",
      "Max absolute difference L:  tensor(9.0901)\n"
     ]
    }
   ],
   "source": [
    "O = torch.zeros(N_out, d)\n",
    "L = torch.zeros(N_out, 1)\n",
    "\n",
    "flash_attention_torch(Q, K, V, O, L, N_inp, N_out, d)\n",
    "\n",
    "check_diff(O, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.cuda.jit\n",
    "def attention_numba_spilling(Q, K, V, scale_factor: numba.float32, L, O):\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "    \n",
    "    # These can be in registers\n",
    "    l_i = numba.cuda.local.array((B_r,), inp_dtype)\n",
    "    m_i = numba.cuda.local.array((B_r,), inp_dtype)\n",
    "    O_i = numba.cuda.local.array((B_r, d), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "                O_i[ii, dd] = 0\n",
    "            l_i[ii] = 0\n",
    "            m_i[ii] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scale_factor * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, numba.cuda.blockDim.x):\n",
    "                for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scale_factor * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "                # torch.maximum(m_i, S_i.max(dim=-1, keepdim=True).values)\n",
    "                # this needs to use the parallel reduction pattern\n",
    "                m = m_i[ii]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii][jj])\n",
    "                m_i[ii] = m\n",
    "                l = math.exp(last_m - m) * l_i[ii]\n",
    "\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    O_i[ii, dd] *= math.exp(last_m - m)\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = math.exp(S[ii][jj] - m)  # Cache...\n",
    "                    l += P_ij\n",
    "                    for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                        O_i[ii, dd] += P_ij * V_j[jj, dd]\n",
    "                l_i[ii] = l\n",
    "        numba.cuda.syncthreads()\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):  \n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                O[ii + i * B_r, dd] = O_i[ii, dd] / l_i[ii]\n",
    "            L[ii + i * B_r] = l_i[ii]   \n",
    "        numba.cuda.syncthreads() \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference O:  tensor(5.6624e-07)\n",
      "Max absolute difference L:  tensor(5.2452e-06)\n"
     ]
    }
   ],
   "source": [
    "O_splilling = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "L_splilling = torch.zeros(N_out, device=\"cuda\")\n",
    "tpb = (8, 16)\n",
    "grid = (1,)\n",
    "attention_numba_spilling[grid, tpb](Qc, Kc, Vc, scaling, L_splilling, O_splilling)\n",
    "check_diff(O_splilling.cpu(), L_splilling.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_dim_x = 32\n",
    "block_dim_y = 4\n",
    "B_r = 16\n",
    "o_per_thread_x = d // block_dim_x\n",
    "o_per_thread_y = B_r // block_dim_y\n",
    "\n",
    "@numba.cuda.jit\n",
    "def attention_numba_no_spilling(Q, K, V, scale_factor: numba.float32, L, O):\n",
    "    B_c = 16\n",
    "    \n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "    \n",
    "    # These can be in registers\n",
    "    l_i = numba.cuda.local.array((o_per_thread_y,), inp_dtype)\n",
    "    m_i = numba.cuda.local.array((o_per_thread_y,), inp_dtype)\n",
    "    O_i = numba.cuda.local.array((o_per_thread_y, o_per_thread_x), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "                O_i[ii//block_dim_y, dd//block_dim_x] = 0\n",
    "            l_i[ii//block_dim_y] = 0\n",
    "            m_i[ii//block_dim_y] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scale_factor * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, numba.cuda.blockDim.x):\n",
    "                for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scale_factor * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "                # torch.maximum(m_i, S_i.max(dim=-1, keepdim=True).values)\n",
    "                # this needs to use the parallel reduction pattern\n",
    "                m = m_i[ii//block_dim_y]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii][jj])\n",
    "                m_i[ii//block_dim_y] = m\n",
    "                l = math.exp(last_m - m) * l_i[ii//block_dim_y]\n",
    "\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    O_i[ii//block_dim_y, dd//block_dim_x] *= math.exp(last_m - m)\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = math.exp(S[ii][jj] - m)  # Cache...\n",
    "                    l += P_ij\n",
    "                    for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                        O_i[ii//block_dim_y, dd//block_dim_x] += P_ij * V_j[jj, dd]\n",
    "                l_i[ii//block_dim_y] = l\n",
    "        numba.cuda.syncthreads()\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):  \n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                O[ii + i * B_r, dd] = O_i[ii//block_dim_y, dd//block_dim_x] / l_i[ii//block_dim_y]\n",
    "            L[ii + i * B_r] = l_i[ii//block_dim_y]   \n",
    "        numba.cuda.syncthreads() \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference O:  tensor(5.6624e-07)\n",
      "Max absolute difference L:  tensor(5.2452e-06)\n"
     ]
    }
   ],
   "source": [
    "O_no_spilling = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "L_no_spilling = torch.zeros(N_out, device=\"cuda\")\n",
    "tpb = (block_dim_x, block_dim_y)\n",
    "grid = (1,)\n",
    "attention_numba_no_spilling[grid, tpb](Qc, Kc, Vc, scaling, L_no_spilling, O_no_spilling)\n",
    "check_diff(O_no_spilling.cpu(), L_no_spilling.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cuda\n",
    "\n",
    "### Register spilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/sagemaker-user/.cache/torch_extensions/py312_cu126 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module flash_attention_v1, skipping build step...\n",
      "Loading extension module flash_attention_v1...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# cuda_src_path = \"./flash_attention.cu\"\n",
    "cuda_src_path = \"./flash_attention.cu\"\n",
    "cuda_src = Path(cuda_src_path).read_text()\n",
    "fname = 'flash_attention'\n",
    "cpp_src = get_sig(fname, cuda_src)\n",
    "module_cuda_spilling = load_cuda(cuda_src, cpp_src, [fname], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference O:  tensor(2.9802e-07)\n",
      "Max absolute difference L:  tensor(1.9073e-06)\n"
     ]
    }
   ],
   "source": [
    "O_cuda_spilling, L_cuda_spilling = getattr(module_cuda_spilling, fname)(Kc, Qc, Vc)\n",
    "check_diff(O_cuda_spilling.cpu(), L_cuda_spilling.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Registers spilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/sagemaker-user/.cache/torch_extensions/py312_cu126 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sagemaker-user/.cache/torch_extensions/py312_cu126/flash_attention/build.ninja...\n",
      "/opt/conda/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2073: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module flash_attention...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /opt/conda/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=flash_attention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -isystem /opt/conda/include -isystem /opt/conda/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.12/site-packages/torch/include -isystem /opt/conda/include -isystem /opt/conda/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/sagemaker-user/.cache/torch_extensions/py312_cu126/flash_attention/main.cpp -o main.o \n",
      "[2/3] /opt/conda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -ccbin /opt/conda/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=flash_attention -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -isystem /opt/conda/include -isystem /opt/conda/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.12/site-packages/torch/include -isystem /opt/conda/include -isystem /opt/conda/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -Xptxas -O3 -Xcompiler -O3 -std=c++17 -c /home/sagemaker-user/.cache/torch_extensions/py312_cu126/flash_attention/cuda.cu -o cuda.cuda.o \n",
      "nvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\n",
      "[3/3] /opt/conda/bin/x86_64-conda-linux-gnu-c++ main.o cuda.cuda.o -shared -L/opt/conda/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib64 -lcudart -o flash_attention.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module flash_attention...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "cuda_src_path = \"./flash_attention_spilling.cu\"\n",
    "cuda_src = Path(cuda_src_path).read_text()\n",
    "fname = 'flash_attention_spilling'\n",
    "cpp_src = get_sig(fname, cuda_src)\n",
    "module_not_spilling = load_cuda(cuda_src, cpp_src, [fname], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference O:  tensor(1.8869)\n",
      "Max absolute difference L:  tensor(11.8267)\n"
     ]
    }
   ],
   "source": [
    "O4, L4 = getattr(module_not_spilling, fname)(Kc, Qc, Vc)\n",
    "check_diff(O4.cpu(), L4.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/sagemaker-user/.cache/torch_extensions/py312_cu126 as PyTorch extensions root...\n",
      "The input conditions for extension module flash_attention_official have changed. Bumping to version 1 and re-building as flash_attention_official_v1...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/sagemaker-user/.cache/torch_extensions/py312_cu126/flash_attention_official/build.ninja...\n",
      "/opt/conda/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2073: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module flash_attention_official_v1...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] /opt/conda/bin/x86_64-conda-linux-gnu-c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=flash_attention_official_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -isystem /opt/conda/include -isystem /opt/conda/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.12/site-packages/torch/include -isystem /opt/conda/include -isystem /opt/conda/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -c /home/sagemaker-user/.cache/torch_extensions/py312_cu126/flash_attention_official/main.cpp -o main.o \n",
      "[2/3] /opt/conda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -ccbin /opt/conda/bin/x86_64-conda-linux-gnu-cc -DTORCH_EXTENSION_NAME=flash_attention_official_v1 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1018\\\" -isystem /opt/conda/include -isystem /opt/conda/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.12/site-packages/torch/include -isystem /opt/conda/include -isystem /opt/conda/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 -Xptxas -O3 -Xcompiler -O3 -std=c++17 -c /home/sagemaker-user/.cache/torch_extensions/py312_cu126/flash_attention_official/cuda.cu -o cuda.cuda.o \n",
      "nvcc warning : incompatible redefinition for option 'compiler-bindir', the last value of this option was used\n",
      "[3/3] /opt/conda/bin/x86_64-conda-linux-gnu-c++ main.o cuda.cuda.o -shared -L/opt/conda/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib64 -lcudart -o flash_attention_official_v1.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module flash_attention_official_v1...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "cuda_src_path = \"./flash_attention_official.cu\"\n",
    "cuda_src = Path(cuda_src_path).read_text()\n",
    "fname = 'flash_attention_official'\n",
    "cpp_src = get_sig(fname, cuda_src)\n",
    "module_not_spilling = load_cuda(cuda_src, cpp_src, [fname], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference O:  tensor(0.9175)\n",
      "Max absolute difference L:  tensor(11.3697)\n"
     ]
    }
   ],
   "source": [
    "O_official, L_official = getattr(module_not_spilling, fname)(Kc, Qc, Vc)\n",
    "check_diff(O_official.cpu(), L_official.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get registers and shared memory info\n",
    "To calculare block sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COMPUTE_CAPABILITY_MAJOR', 'COMPUTE_CAPABILITY_MINOR', 'PCI_BUS_ID', 'PCI_DEVICE_ID', 'PCI_DOMAIN_ID', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'attributes', 'compute_capability', 'from_identity', 'get_device_identity', 'get_primary_context', 'id', 'name', 'primary_context', 'release_primary_context', 'reset', 'supports_float16', 'uuid']\n",
      "b'NVIDIA A10G'\n",
      "(8, 6)\n",
      "{'pci_domain_id': 0, 'pci_bus_id': 0, 'pci_device_id': 30}\n",
      "<CUDA context c_void_p(94262389834336) of device 0>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'get_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(dev\u001b[38;5;241m.\u001b[39mget_device_identity())\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(dev\u001b[38;5;241m.\u001b[39mget_primary_context())\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_identity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_info\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'get_info'"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "dev = cuda.get_current_device()\n",
    "print(dir(dev))\n",
    "print(dev.name)\n",
    "print(dev.compute_capability)\n",
    "print(dev.get_device_identity())\n",
    "print(dev.get_primary_context())\n",
    "print(dev.get_device_identity().get_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA A10G\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch._C._CudaDeviceProperties' object has no attribute 'shared_memory_per_block'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m props \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_properties(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice:\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax shared memory per block:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mprops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_memory_per_block\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax registers per block:\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m.\u001b[39mregs_per_block)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarp size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m.\u001b[39mwarp_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch._C._CudaDeviceProperties' object has no attribute 'shared_memory_per_block'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "props = torch.cuda.get_device_properties(0)\n",
    "print(\"Device:\", props.name)\n",
    "print(\"Max shared memory per block:\", props.shared_memory_per_block, \"bytes\")\n",
    "print(\"Max registers per block:\", props.regs_per_block)\n",
    "print(\"Warp size:\", props.warp_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L2_cache_size',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_pybind11_conduit_v1_',\n",
       " 'gcnArchName',\n",
       " 'is_integrated',\n",
       " 'is_multi_gpu_board',\n",
       " 'major',\n",
       " 'max_threads_per_multi_processor',\n",
       " 'minor',\n",
       " 'multi_processor_count',\n",
       " 'name',\n",
       " 'regs_per_multiprocessor',\n",
       " 'total_memory',\n",
       " 'uuid',\n",
       " 'warp_size']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mod for mod in dir(props)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 4 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "ename": "NotDefinedError",
     "evalue": "Failed in cuda mode pipeline (step: analyzing bytecode)\n\u001b[1m\u001b[1mThe compiler failed to analyze the bytecode. Variable 'l_cur' is not defined.\n\u001b[1m\nFile \"../../../../../../tmp/ipykernel_5907/2646925363.py\", line 58:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n\u001b[0m\u001b[1mDuring: Pass translate_bytecode\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotDefinedError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 100\u001b[0m\n\u001b[1;32m     96\u001b[0m K \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(N_inp, d)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     97\u001b[0m V \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(N_inp, d)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m--> 100\u001b[0m custom_fa \u001b[38;5;241m=\u001b[39m \u001b[43mflash_attention_numba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m torch_fa \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(Q \u001b[38;5;241m@\u001b[39m K\u001b[38;5;241m.\u001b[39mT, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m@\u001b[39m V\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misclose(custom_fa,  torch_fa )\u001b[38;5;241m.\u001b[39mall():\n",
      "Cell \u001b[0;32mIn[7], line 88\u001b[0m, in \u001b[0;36mflash_attention_numba\u001b[0;34m(Q, K, V)\u001b[0m\n\u001b[1;32m     86\u001b[0m tpb \u001b[38;5;241m=\u001b[39m d, B_r\n\u001b[1;32m     87\u001b[0m blocks \u001b[38;5;241m=\u001b[39m cdiv(d,tpb[\u001b[38;5;241m0\u001b[39m]), cdiv(d,tpb[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 88\u001b[0m \u001b[43mflash_attention_k_numba\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtpb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdyn_shared_mem_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mO\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m O\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:539\u001b[0m, in \u001b[0;36m_LaunchConfiguration.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgriddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblockdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msharedmem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:681\u001b[0m, in \u001b[0;36mCUDADispatcher.call\u001b[0;34m(self, args, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[1;32m    679\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverloads\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[43m_dispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m kernel\u001b[38;5;241m.\u001b[39mlaunch(args, griddim, blockdim, stream, sharedmem)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:689\u001b[0m, in \u001b[0;36mCUDADispatcher._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kws\n\u001b[1;32m    688\u001b[0m argtypes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypeof_pyval(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margtypes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:932\u001b[0m, in \u001b[0;36mCUDADispatcher.compile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_compile:\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilation disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 932\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[43m_Kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargetoptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# We call bind to force codegen, so that there is a cubin to cache\u001b[39;00m\n\u001b[1;32m    934\u001b[0m kernel\u001b[38;5;241m.\u001b[39mbind()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:83\u001b[0m, in \u001b[0;36m_Kernel.__init__\u001b[0;34m(self, py_func, argtypes, link, debug, lineinfo, inline, fastmath, extensions, max_registers, opt, device)\u001b[0m\n\u001b[1;32m     77\u001b[0m nvvm_options \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastmath\u001b[39m\u001b[38;5;124m'\u001b[39m: fastmath,\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     80\u001b[0m }\n\u001b[1;32m     82\u001b[0m cc \u001b[38;5;241m=\u001b[39m get_current_device()\u001b[38;5;241m.\u001b[39mcompute_capability\n\u001b[0;32m---> 83\u001b[0m cres \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlineinfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineinfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfastmath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfastmath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnvvm_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnvvm_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m tgt_ctx \u001b[38;5;241m=\u001b[39m cres\u001b[38;5;241m.\u001b[39mtarget_context\n\u001b[1;32m     91\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpy_func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/cuda/compiler.py:196\u001b[0m, in \u001b[0;36mcompile_cuda\u001b[0;34m(pyfunc, return_type, args, debug, lineinfo, inline, fastmath, nvvm_options, cc)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtarget_extension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m target_override\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m target_override(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 196\u001b[0m     cres \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_extra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypingctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypingctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtargetctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargetctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCUDACompiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m library \u001b[38;5;241m=\u001b[39m cres\u001b[38;5;241m.\u001b[39mlibrary\n\u001b[1;32m    206\u001b[0m library\u001b[38;5;241m.\u001b[39mfinalize()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler.py:739\u001b[0m, in \u001b[0;36mcompile_extra\u001b[0;34m(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compiler entry point\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03mParameter\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;124;03m    compiler pipeline\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    737\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m pipeline_class(typingctx, targetctx, library,\n\u001b[1;32m    738\u001b[0m                           args, return_type, flags, \u001b[38;5;28mlocals\u001b[39m)\n\u001b[0;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_extra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler.py:439\u001b[0m, in \u001b[0;36mCompilerBase.compile_extra\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlifted \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlifted_from \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_bytecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler.py:505\u001b[0m, in \u001b[0;36mCompilerBase._compile_bytecode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;124;03mPopulate and run pipeline for bytecode input\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfunc_ir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler.py:484\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mfail_reason \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_final_pipeline:\n\u001b[0;32m--> 484\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CompilerError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll available pipelines exhausted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler.py:473\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    471\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler_machinery.py:367\u001b[0m, in \u001b[0;36mPassManager.run\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    364\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m mode pipeline (step: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \\\n\u001b[1;32m    365\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_name, pass_desc)\n\u001b[1;32m    366\u001b[0m patched_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_patch_error(msg, e)\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m patched_exception\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler_machinery.py:356\u001b[0m, in \u001b[0;36mPassManager.run\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    354\u001b[0m pass_inst \u001b[38;5;241m=\u001b[39m _pass_registry\u001b[38;5;241m.\u001b[39mget(pss)\u001b[38;5;241m.\u001b[39mpass_inst\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pass_inst, CompilerPass):\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runPass\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_inst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLegacy pass in use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler_machinery.py:311\u001b[0m, in \u001b[0;36mPassManager._runPass\u001b[0;34m(self, index, pss, internal_state)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m check(pss\u001b[38;5;241m.\u001b[39mrun_initialization, internal_state)\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SimpleTimer() \u001b[38;5;28;01mas\u001b[39;00m pass_time:\n\u001b[0;32m--> 311\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SimpleTimer() \u001b[38;5;28;01mas\u001b[39;00m finalize_time:\n\u001b[1;32m    313\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m check(pss\u001b[38;5;241m.\u001b[39mrun_finalizer, internal_state)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/compiler_machinery.py:272\u001b[0m, in \u001b[0;36mPassManager._runPass.<locals>.check\u001b[0;34m(func, compiler_state)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck\u001b[39m(func, compiler_state):\n\u001b[0;32m--> 272\u001b[0m     mangled \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mangled \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    274\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilerPass implementations should return True/False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilerPass with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m did not.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/untyped_passes.py:86\u001b[0m, in \u001b[0;36mTranslateByteCode.run_pass\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     84\u001b[0m bc \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     85\u001b[0m interp \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mInterpreter(func_id)\n\u001b[0;32m---> 86\u001b[0m func_ir \u001b[38;5;241m=\u001b[39m \u001b[43minterp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpret\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc_ir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func_ir\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/interpreter.py:1407\u001b[0m, in \u001b[0;36mInterpreter.interpret\u001b[0;34m(self, bytecode)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;66;03m# Interpret loop\u001b[39;00m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inst, kws \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_inst():\n\u001b[0;32m-> 1407\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkws\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PYVERSION \u001b[38;5;129;01min\u001b[39;00m ((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m), (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m), (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m13\u001b[39m)):\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# Insert end of try markers\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_try_blocks()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numba/core/interpreter.py:1830\u001b[0m, in \u001b[0;36mInterpreter._dispatch\u001b[0;34m(self, inst, kws)\u001b[0m\n\u001b[1;32m   1828\u001b[0m err \u001b[38;5;241m=\u001b[39m errors\u001b[38;5;241m.\u001b[39mNotDefinedError(e\u001b[38;5;241m.\u001b[39mname, loc\u001b[38;5;241m=\u001b[39mloc)\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mFULL_TRACEBACKS:\n\u001b[0;32m-> 1830\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1832\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling op: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | offset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39moffset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNotDefinedError\u001b[0m: Failed in cuda mode pipeline (step: analyzing bytecode)\n\u001b[1m\u001b[1mThe compiler failed to analyze the bytecode. Variable 'l_cur' is not defined.\n\u001b[1m\nFile \"../../../../../../tmp/ipykernel_5907/2646925363.py\", line 58:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n\u001b[0m\u001b[1mDuring: Pass translate_bytecode\u001b[0m"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def flash_attention_k_numba(Q, K, V, O, N_inp, N_out, d):\n",
    "    # 1D block of size d for Q\n",
    "    # K and V are not transponse\n",
    "    # Block is B_r x d (while for K, V we want B_c x d)\n",
    "    cbi, cbd, tid = cuda.blockIdx, cuda.blockDim, cuda.threadIdx\n",
    "    tc = tid.x\n",
    "    tr = tid.y\n",
    "    r = cbi.y * cbd.y + tr\n",
    "    c = cbi.x * cbd.x + tc\n",
    "\n",
    "    B_r: int = 1\n",
    "    T_r = math.ceil(N_out/B_r)\n",
    "    B_c: int = 1\n",
    "    T_c = math.ceil(N_inp/B_c)\n",
    "\n",
    "\n",
    "    shar = cuda.shared.array(0, dtype=np.float32)\n",
    "    Qs = shar[: B_r * d]\n",
    "    Ks = shar[B_r * d: (B_r * d) + d * B_c]\n",
    "    Vs = shar[(B_r * d) + d * B_c: (B_r * d) + (d * B_c)*2]\n",
    "    Ss = shar[(B_r * d) + (d * B_c) * 2: (B_r * d) + (d * B_c)*2 + B_r * B_c]\n",
    "    Ps = shar[(B_r * d) + (d * B_c) * 2 + B_r * B_c:  2*(B_r * d) + (d * B_c) * 2 + B_r * B_c]\n",
    "    Os = shar[2 * (B_r * d) + (d * B_c) * 2 + B_r * B_c: 3 * (B_r * d) + (d * B_c) * 2 + B_r * B_c]\n",
    "\n",
    "\n",
    "    L_i = torch.zeros(B_r, 1)\n",
    "    m_i = torch.full((B_r, 1), -math.inf)\n",
    "    last_m_i = torch.full((B_r, 1), -math.inf)\n",
    "\n",
    "    # Load Qs on chip\n",
    "    Qs[tr * cbd.x + tc ] = Q[r, c] if r < N and c < d else 0.\n",
    "    cuda.syncthreads()\n",
    "\n",
    "    for j in range(T_c):\n",
    "        # Columns loaded contigously (column major/transpose)\n",
    "        Ks[j*d + tc] = K[c, j * B_c + tc] if c < d and j * B_c + tc < d else 0.\n",
    "        Vs[j*d + tc] = V[c, j * B_c + tc] if c < d and j * B_c + tc < d else 0.\n",
    "        cuda.syncthreads()\n",
    "        # Compute S_ij = Q_i K_j^T\n",
    "        for _d in range(d): \n",
    "            Ss[tr*d + tc] += Qs[tr*d + _d] * Ks[j*d + _d]\n",
    "        cuda.syncthreads()\n",
    "        Ss_max = torch.full((B_r, 1), -math.inf)\n",
    "\n",
    "        # todo must be by row\n",
    "        for idx in range(B_r * B_c):  # or actual length of Ss\n",
    "            if Ss[idx] > Ss_max:\n",
    "                Ss_max = Ss[idx]\n",
    "                m_i[Idx] = ...\n",
    "        m_new = max(m_cur, Ss_max) \n",
    "\n",
    "        if j == tc:\n",
    "            for idx in range(B_r * B_c):  # or the actual length of Ss\n",
    "                Ps[idx] = math.exp(Ss[idx] - m_new)\n",
    "\n",
    "        cuda.syncthreads()\n",
    "        l_new = math.exp(m_cur - m_new) * l_cur\n",
    "        for idx in range(B_r * B_c):\n",
    "            l_new +=  Ps[idx]\n",
    "        l_cur = l_new\n",
    "\n",
    "        Os[c] = Os[c] * math.exp(m_cur - m_new)\n",
    "        \n",
    "        for _d in range(d): \n",
    "            Os[c] += Ps[_d] * Vs[_d]\n",
    "\n",
    "        cuda.syncthreads()\n",
    "\n",
    "  \n",
    "    if r < N and c < d: O[r, c] = Os[c] / l_new\n",
    "\n",
    "def flash_attention_numba(Q, K, V):\n",
    "    N_out ,d  = Q.shape\n",
    "    N_inp, kw = K.shape\n",
    "    vr, vw = V.shape\n",
    "    assert d==kw, \"Size mismatch!\"\n",
    "    assert d==vw, \"Size mismatch!\"\n",
    "    assert N_inp==vr, \"Size mismatch!\"\n",
    "    O = torch.zeros(N_out, d, dtype=Q.dtype, device=Q.device)\n",
    "\n",
    "    B_r: int = 1\n",
    "    B_c: int = 1\n",
    "    \n",
    "    dyn_shared_mem_size =  3*(B_r * d) + (d * B_c) * 2 + (B_r * B_c)\n",
    "    tpb = d, B_r\n",
    "    blocks = cdiv(d,tpb[0]), cdiv(d,tpb[1])\n",
    "    flash_attention_k_numba[blocks, tpb, 0, dyn_shared_mem_size](\n",
    "        ca(Q), ca(K), ca(V), ca(O), N_inp, N_out, d\n",
    "    ) \n",
    "    return O\n",
    "\n",
    "N_out, N_inp, d = 2, 3, 4\n",
    "\n",
    "Q = torch.rand(N_out, d).contiguous().cuda()\n",
    "K = torch.rand(N_inp, d).contiguous().cuda()\n",
    "V = torch.rand(N_inp, d).contiguous().cuda()\n",
    "\n",
    "\n",
    "custom_fa = flash_attention_numba(Q, K, V)\n",
    "torch_fa = torch.softmax(Q @ K.T, dim=-1) @ V\n",
    "\n",
    "if not torch.isclose(custom_fa,  torch_fa ).all():\n",
    "    print(\"Mismatch\")\n",
    "    print(f\"\\n{custom_fa}\")\n",
    "    print(f\"\\n{torch_fa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9416, 0.5529, 0.7670],\n",
       "        [0.9408, 0.5372, 0.7598]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, d = 2, 3\n",
    "\n",
    "Q = torch.rand(N, d).contiguous().cuda()\n",
    "K = torch.rand(N, d).contiguous().cuda()\n",
    "V = torch.rand(N, d).contiguous().cuda()\n",
    "\n",
    "torch.softmax(Q @ K.T, dim=-1) @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul_k_numba(m, n, out, tw):\n",
    "    cbi,cbd,tid = cuda.blockIdx,cuda.blockDim,cuda.threadIdx\n",
    "    tc,tr = tid.x,tid.y\n",
    "    r,c = cbi.y * cbd.y + tr, cbi.x * cbd.x + tc\n",
    "    h,k  = m.shape\n",
    "    k2,w = n.shape\n",
    "\n",
    "    shar = cuda.shared.array(0, dtype=np.float32)\n",
    "    ms,ns = shar[:tw*tw],shar[tw*tw:2*tw*tw]\n",
    "\n",
    "    p = np.float32(0.0)\n",
    "    for ph in range(math.ceil(k/tw)):\n",
    "        idx = ph*tw\n",
    "        ms[tr*tw+tc] = m[r, tc+idx] if r<h and idx+tc<k else 0.\n",
    "        ns[tr*tw+tc] = n[tr+idx, c] if c<w and idx+tr<k else 0.\n",
    "        cuda.syncthreads()\n",
    "        for i in range(tw): p += ms[tr*tw+i] * ns[i*tw+tc]\n",
    "        cuda.syncthreads()\n",
    "    if r < h and c < w: out[r, c] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_2d_numba(m, n, tw=16):\n",
    "    h,k  = m.shape\n",
    "    k2,w = n.shape\n",
    "    assert k==k2, \"Size mismatch!\"\n",
    "    out = torch.zeros(h, w, dtype=m.dtype, device=m.device)\n",
    "    dyn_shared_mem_size = 2 * tw * tw * 4\n",
    "    tpb = tw,tw\n",
    "    blocks = cdiv(w,tpb[0]), cdiv(h,tpb[1])\n",
    "    matmul_k_numba[blocks, tpb, 0, dyn_shared_mem_size](ca(m), ca(n), ca(out), tw) \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 5 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "N, L, M = 12, 34, 65\n",
    "\n",
    "Q = torch.rand(N, L).contiguous().cuda()\n",
    "K = torch.rand(L, M).contiguous().cuda()\n",
    "\n",
    "torch.isclose(matmul_2d_numba(Q, K), Q@K).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, L, M = 12, 34, 65\n",
    "\n",
    "Q = torch.rand(N, L).contiguous().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 s  68.3 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "matmul_2d_numba(Q,K)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.6 s  20.9 s per loop (mean  std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "Q@K\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
