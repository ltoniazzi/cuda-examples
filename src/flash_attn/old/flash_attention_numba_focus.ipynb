{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash attention in python, numba and cuda\n",
    "\n",
    "todo: registers calculation, show spills and run both versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba.cuda import as_cuda_array as ca\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import sys, os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "sys.path.insert(0, '../..')\n",
    "from utils import load_cuda, cuda_begin, cdiv, get_sig\n",
    "\n",
    "def cdiv(a,b):\n",
    "    \"Int ceiling division of `a` over `b`\"\n",
    "    return (a+b-1)//b\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tensors\n",
    "N_inp = 32\n",
    "N_out = 32\n",
    "d = 128\n",
    "Q = torch.randn(N_out, d).contiguous()\n",
    "K = torch.randn(N_inp, d).contiguous()\n",
    "V = torch.randn(N_inp, d).contiguous()\n",
    "Kc = K.to(\"cuda\")\n",
    "Qc = Q.to(\"cuda\")\n",
    "Vc = V.to(\"cuda\")\n",
    "scaling = 1.0 / math.sqrt(d)\n",
    "\n",
    "# Get expected O\n",
    "O_expected = torch.softmax(Q @ K.T * scaling, dim=-1) @ V\n",
    "# Get expected L\n",
    "S = (Q @ K.T) * scaling  # shape: (N_out, N_in)\n",
    "max_per_row, _ = torch.max(S, dim=1, keepdim=True)  # shape: (N_out, 1)\n",
    "exp_shifted = torch.exp(S - max_per_row)  # shape: (N_out, N_in)\n",
    "L_expected = torch.sum(exp_shifted, dim=1)\n",
    "\n",
    "def check_diff(O, L):\n",
    "    print(\"Max absolute difference O: \", (O-O_expected).abs().max())\n",
    "    print(\"Max absolute difference L: \", (L-L_expected).abs().max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.cuda.jit\n",
    "def attention_numba(Q, K, V, scale_factor: numba.float32, L, O):\n",
    "    B_c = 16\n",
    "    B_r = 16\n",
    "    T_c = (N_inp + B_c - 1) // B_c\n",
    "    T_r = (N_out + B_r - 1) // B_r\n",
    "    inp_dtype = K.dtype\n",
    "    tid_x = numba.cuda.threadIdx.x\n",
    "    tid_y = numba.cuda.threadIdx.y\n",
    "\n",
    "    Q_i = numba.cuda.shared.array((B_r, d), inp_dtype)\n",
    "    K_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    V_j = numba.cuda.shared.array((B_c, d), inp_dtype)\n",
    "    S = numba.cuda.shared.array((B_r, B_c), inp_dtype)\n",
    "    \n",
    "    # These can be in registers\n",
    "    l_i = numba.cuda.local.array((1,), inp_dtype)\n",
    "    m_i = numba.cuda.local.array((1,), inp_dtype)\n",
    "    O_i = numba.cuda.local.array((1, 4), inp_dtype)\n",
    "\n",
    "                 \n",
    "    for i in range(T_r):\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                Q_i[ii, dd] = Q[ii + i * B_r, dd]\n",
    "                O_i[ii//16, dd//32] = 0\n",
    "            l_i[ii//16] = 0\n",
    "            m_i[ii//16] = -math.inf\n",
    "        numba.cuda.syncthreads()\n",
    "\n",
    "        for j in range(T_c):\n",
    "            for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    K_j[jj, dd] = K[jj + j * B_c, dd]\n",
    "                    V_j[jj, dd] = V[jj + j * B_c, dd]\n",
    "\n",
    "            # S[ii][jj] = scale_factor * (Q_i @ K_j.T)\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_x, B_r, numba.cuda.blockDim.x):\n",
    "                for jj in range(tid_y, B_c, numba.cuda.blockDim.y):\n",
    "                    S_ij = 0\n",
    "                    for dd in range(d):\n",
    "                        S_ij += Q_i[ii, dd] * K_j[jj, dd]\n",
    "                    S_ij = scale_factor * S_ij\n",
    "                    S[ii][jj] = S_ij\n",
    "\n",
    "            numba.cuda.syncthreads()\n",
    "            for ii in range(tid_y, B_r, numba.cuda.blockDim.y):\n",
    "                # torch.maximum(m_i, S_i.max(dim=-1, keepdim=True).values)\n",
    "                # this needs to use the parallel reduction pattern\n",
    "                m = m_i[ii//16]\n",
    "                last_m = m\n",
    "                for jj in range(B_c):\n",
    "                    m = max(m, S[ii][jj])\n",
    "                m_i[ii//16] = m\n",
    "                l = math.exp(last_m - m) * l_i[ii//16]\n",
    "\n",
    "                for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                    O_i[ii//16, dd//32] *= math.exp(last_m - m)\n",
    "                for jj in range(B_c):\n",
    "                    P_ij = math.exp(S[ii][jj] - m)  # Cache...\n",
    "                    l += P_ij\n",
    "                    for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                        O_i[ii//16, dd//32] += P_ij * V_j[jj, dd]\n",
    "                l_i[ii//16] = l\n",
    "        numba.cuda.syncthreads()\n",
    "        for ii in range(tid_y, B_r, numba.cuda.blockDim.y):  \n",
    "            for dd in range(tid_x, d, numba.cuda.blockDim.x):\n",
    "                O[ii + i * B_r, dd] = O_i[ii//16, dd//32] / l_i[ii//16]\n",
    "            L[ii + i * B_r] = l_i[ii//16]   \n",
    "        numba.cuda.syncthreads() \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference O:  tensor(5.3644e-07)\n",
      "Max absolute difference L:  tensor(7.6294e-06)\n"
     ]
    }
   ],
   "source": [
    "O2 = torch.zeros(N_out, d, device=\"cuda\").contiguous()\n",
    "L2 = torch.zeros(N_out, device=\"cuda\")\n",
    "tpb = (32, 16)\n",
    "# tpb = (1,) # works\n",
    "grid = (1,)\n",
    "# attention_numba[grid, tpb](Qc, Kc, Vc, scaling, L2, O2)\n",
    "torch.cuda.synchronize()\n",
    "attention_numba[grid, tpb](Qc, Kc, Vc, scaling, L2, O2)\n",
    "\n",
    "check_diff(O2.cpu(), L2.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max absolute difference O:  tensor(1.0513)\n",
      "Max absolute difference L:  tensor(7.6294e-06)\n"
     ]
    }
   ],
   "source": [
    "check_diff(O2.cpu(), L2.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registers not spilling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
